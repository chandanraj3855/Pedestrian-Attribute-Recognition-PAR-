{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37536,"status":"ok","timestamp":1717868745590,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"rTeg2X1PVIgV","outputId":"b27ddfcf-8454-4647-aec1-bc75dc0e9d17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9779,"status":"ok","timestamp":1717852615640,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"kD7Wwi-7YJHZ","outputId":"7b222730-8193-4528-c33c-8282cb05910a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}],"source":["!pip install tensorflow pandas\n"]},{"cell_type":"code","source":["import os\n","from sklearn.model_selection import train_test_split\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')"],"metadata":{"id":"AELsuGhTg4qi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","train_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"collapsed":true,"id":"_IPZuJmChM3u","executionInfo":{"status":"ok","timestamp":1717852659360,"user_tz":-330,"elapsed":1398,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"a7b2114f-7c9d-4e73-9197-483beaf0b70e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      0   1   2   3   4   5   6   7   8   9   ...  40  41  42  43  44  45  46  \\\n","0      1   1   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   1   \n","1      2   0   0   0   0   0   0   0   1   0  ...   1   0   1   0   0   0   1   \n","2      3   0   0   0   0   0   1   0   0   0  ...   0   1   0   0   0   0   1   \n","3      4   0   0   0   0   0   0   0   1   0  ...   0   0   0   1   1   0   0   \n","4      5   1   0   0   0   0   0   0   0   0  ...   1   0   1   0   1   0   0   \n","..   ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..   \n","595  596   0   0   1   0   0   0   0   0   0  ...   0   0   0   0   0   0   1   \n","596  597   0   0   0   0   0   0   0   0   1  ...   1   0   0   1   0   0   1   \n","597  598   0   0   0   0   0   0   0   1   0  ...   0   0   0   1   0   0   1   \n","598  599   0   0   0   0   0   0   0   0   1  ...   1   0   0   1   0   0   1   \n","599  600   0   0   1   0   0   0   0   0   0  ...   0   1   0   0   0   0   1   \n","\n","     47  48  49  \n","0     1   0   0  \n","1     1   0   0  \n","2     0   1   0  \n","3     0   1   0  \n","4     1   0   0  \n","..   ..  ..  ..  \n","595   0   1   0  \n","596   1   0   0  \n","597   0   1   0  \n","598   0   1   0  \n","599   0   1   0  \n","\n","[600 rows x 50 columns]"],"text/html":["\n","  <div id=\"df-042157a6-7321-4309-bc15-2aab55e867ad\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>596</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>597</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>598</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>599</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>599</th>\n","      <td>600</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>600 rows Ã— 50 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-042157a6-7321-4309-bc15-2aab55e867ad')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-042157a6-7321-4309-bc15-2aab55e867ad button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-042157a6-7321-4309-bc15-2aab55e867ad');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-99e222f0-d6f2-4ce6-95e3-bb475c897235\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99e222f0-d6f2-4ce6-95e3-bb475c897235')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-99e222f0-d6f2-4ce6-95e3-bb475c897235 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_df"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Extract image names and labels\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","labels[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UgvGugHiXIW","executionInfo":{"status":"ok","timestamp":1717852737549,"user_tz":-330,"elapsed":574,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"298b5c4a-47fb-44d1-c788-6ed951231079"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 1, 0, 0])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["image_names = train_df.iloc[:, 0].astype(str).values\n","image_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8qNkaRhiyci","executionInfo":{"status":"ok","timestamp":1717852741975,"user_tz":-330,"elapsed":569,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"198689c7-90e8-4e12-9196-0ab044bbc1d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n","       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n","       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n","       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n","       '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56',\n","       '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67',\n","       '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78',\n","       '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89',\n","       '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100',\n","       '101', '102', '103', '104', '105', '106', '107', '108', '109',\n","       '110', '111', '112', '113', '114', '115', '116', '117', '118',\n","       '119', '120', '121', '122', '123', '124', '125', '126', '127',\n","       '128', '129', '130', '131', '132', '133', '134', '135', '136',\n","       '137', '138', '139', '140', '141', '142', '143', '144', '145',\n","       '146', '147', '148', '149', '150', '151', '152', '153', '154',\n","       '155', '156', '157', '158', '159', '160', '161', '162', '163',\n","       '164', '165', '166', '167', '168', '169', '170', '171', '172',\n","       '173', '174', '175', '176', '177', '178', '179', '180', '181',\n","       '182', '183', '184', '185', '186', '187', '188', '189', '190',\n","       '191', '192', '193', '194', '195', '196', '197', '198', '199',\n","       '200', '201', '202', '203', '204', '205', '206', '207', '208',\n","       '209', '210', '211', '212', '213', '214', '215', '216', '217',\n","       '218', '219', '220', '221', '222', '223', '224', '225', '226',\n","       '227', '228', '229', '230', '231', '232', '233', '234', '235',\n","       '236', '237', '238', '239', '240', '241', '242', '243', '244',\n","       '245', '246', '247', '248', '249', '250', '251', '252', '253',\n","       '254', '255', '256', '257', '258', '259', '260', '261', '262',\n","       '263', '264', '265', '266', '267', '268', '269', '270', '271',\n","       '272', '273', '274', '275', '276', '277', '278', '279', '280',\n","       '281', '282', '283', '284', '285', '286', '287', '288', '289',\n","       '290', '291', '292', '293', '294', '295', '296', '297', '298',\n","       '299', '300', '301', '302', '303', '304', '305', '306', '307',\n","       '308', '309', '310', '311', '312', '313', '314', '315', '316',\n","       '317', '318', '319', '320', '321', '322', '323', '324', '325',\n","       '326', '327', '328', '329', '330', '331', '332', '333', '334',\n","       '335', '336', '337', '338', '339', '340', '341', '342', '343',\n","       '344', '345', '346', '347', '348', '349', '350', '351', '352',\n","       '353', '354', '355', '356', '357', '358', '359', '360', '361',\n","       '362', '363', '364', '365', '366', '367', '368', '369', '370',\n","       '371', '372', '373', '374', '375', '376', '377', '378', '379',\n","       '380', '381', '382', '383', '384', '385', '386', '387', '388',\n","       '389', '390', '391', '392', '393', '394', '395', '396', '397',\n","       '398', '399', '400', '401', '402', '403', '404', '405', '406',\n","       '407', '408', '409', '410', '411', '412', '413', '414', '415',\n","       '416', '417', '418', '419', '420', '421', '422', '423', '424',\n","       '425', '426', '427', '428', '429', '430', '431', '432', '433',\n","       '434', '435', '436', '437', '438', '439', '440', '441', '442',\n","       '443', '444', '445', '446', '447', '448', '449', '450', '451',\n","       '452', '453', '454', '455', '456', '457', '458', '459', '460',\n","       '461', '462', '463', '464', '465', '466', '467', '468', '469',\n","       '470', '471', '472', '473', '474', '475', '476', '477', '478',\n","       '479', '480', '481', '482', '483', '484', '485', '486', '487',\n","       '488', '489', '490', '491', '492', '493', '494', '495', '496',\n","       '497', '498', '499', '500', '501', '502', '503', '504', '505',\n","       '506', '507', '508', '509', '510', '511', '512', '513', '514',\n","       '515', '516', '517', '518', '519', '520', '521', '522', '523',\n","       '524', '525', '526', '527', '528', '529', '530', '531', '532',\n","       '533', '534', '535', '536', '537', '538', '539', '540', '541',\n","       '542', '543', '544', '545', '546', '547', '548', '549', '550',\n","       '551', '552', '553', '554', '555', '556', '557', '558', '559',\n","       '560', '561', '562', '563', '564', '565', '566', '567', '568',\n","       '569', '570', '571', '572', '573', '574', '575', '576', '577',\n","       '578', '579', '580', '581', '582', '583', '584', '585', '586',\n","       '587', '588', '589', '590', '591', '592', '593', '594', '595',\n","       '596', '597', '598', '599', '600'], dtype=object)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"a-xLujeDiXoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QpvIhqePiXz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":3308,"status":"error","timestamp":1717262180533,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"2bv_X2YRYJaF","outputId":"8d5b0af1-e2b6-499a-c857-12d72c9e0dcb"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-ed67c2a36cb6>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-ed67c2a36cb6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","\n","\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","\n","\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0\n","    return img_array\n","\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","\n","def create_cnn_model(input_shape, num_classes):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(num_classes, activation='sigmoid')\n","    ])\n","    return model\n","\n","input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n","model = create_cnn_model(input_shape, NUM_CLASSES)\n","\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                    validation_data=(X_val, y_val))\n","\n","\n","val_loss, val_accuracy = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Accuracy: {val_accuracy}')\n"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the model\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128 * 28 * 28, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# Instantiate and compile the model\n","model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Validation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / (len(val_loader.dataset) * NUM_CLASSES)\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3oRPnua489X3","executionInfo":{"status":"ok","timestamp":1717870838430,"user_tz":-330,"elapsed":2045439,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"03bf3b50-ddf5-4b46-feb8-7508c64ef690"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20, Loss: 0.4970\n","Epoch 2/20, Loss: 0.3894\n","Epoch 3/20, Loss: 0.3618\n","Epoch 4/20, Loss: 0.3356\n","Epoch 5/20, Loss: 0.3127\n","Epoch 6/20, Loss: 0.2919\n","Epoch 7/20, Loss: 0.2618\n","Epoch 8/20, Loss: 0.2334\n","Epoch 9/20, Loss: 0.1981\n","Epoch 10/20, Loss: 0.1582\n","Epoch 11/20, Loss: 0.1336\n","Epoch 12/20, Loss: 0.1142\n","Epoch 13/20, Loss: 0.0978\n","Epoch 14/20, Loss: 0.0888\n","Epoch 15/20, Loss: 0.0832\n","Epoch 16/20, Loss: 0.0819\n","Epoch 17/20, Loss: 0.0697\n","Epoch 18/20, Loss: 0.0721\n","Epoch 19/20, Loss: 0.0614\n","Epoch 20/20, Loss: 0.0634\n","Validation Loss: 0.4058\n","Validation Accuracy: 0.8767\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from sklearn.model_selection import train_test_split\n","\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","\n","\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","\n","\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0\n","    return img_array\n","\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","\n","def create_cnn_model(input_shape, num_classes):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(49, activation='sigmoid'),  # New layer for structured output\n","        tf.keras.layers.Dense(num_classes, activation='sigmoid')  # Output layer\n","    ])\n","    return model\n","\n","input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n","model = create_cnn_model(input_shape, NUM_CLASSES)\n","\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","# Callbacks\n","checkpoint_cb = ModelCheckpoint(\"best_model.h5\", save_best_only=True)\n","early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True)\n","\n","history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n","                    validation_data=(X_val, y_val),\n","                    callbacks=[checkpoint_cb, early_stopping_cb])\n","\n","\n","val_loss, val_accuracy = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Accuracy: {val_accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jad9SPfV1AdC","executionInfo":{"status":"ok","timestamp":1717264205173,"user_tz":-330,"elapsed":1578767,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"27e8ec26-7031-46ca-f593-ea3ae5a71561"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","15/15 [==============================] - ETA: 0s - loss: 0.6083 - accuracy: 0.0000e+00"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15/15 [==============================] - 78s 5s/step - loss: 0.6083 - accuracy: 0.0000e+00 - val_loss: 0.5652 - val_accuracy: 0.0000e+00\n","Epoch 2/20\n","15/15 [==============================] - 86s 6s/step - loss: 0.5376 - accuracy: 0.0000e+00 - val_loss: 0.5082 - val_accuracy: 0.0000e+00\n","Epoch 3/20\n","15/15 [==============================] - 69s 5s/step - loss: 0.4862 - accuracy: 0.0000e+00 - val_loss: 0.4653 - val_accuracy: 0.0000e+00\n","Epoch 4/20\n","15/15 [==============================] - 78s 5s/step - loss: 0.4494 - accuracy: 0.0000e+00 - val_loss: 0.4350 - val_accuracy: 0.0000e+00\n","Epoch 5/20\n","15/15 [==============================] - 76s 5s/step - loss: 0.4267 - accuracy: 0.0021 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n","Epoch 6/20\n","15/15 [==============================] - 73s 5s/step - loss: 0.4118 - accuracy: 0.0021 - val_loss: 0.4062 - val_accuracy: 0.0000e+00\n","Epoch 7/20\n","15/15 [==============================] - 70s 5s/step - loss: 0.4030 - accuracy: 0.0000e+00 - val_loss: 0.3997 - val_accuracy: 0.0000e+00\n","Epoch 8/20\n","15/15 [==============================] - 76s 5s/step - loss: 0.3976 - accuracy: 0.0000e+00 - val_loss: 0.3953 - val_accuracy: 0.0000e+00\n","Epoch 9/20\n","15/15 [==============================] - 79s 5s/step - loss: 0.3943 - accuracy: 0.0000e+00 - val_loss: 0.3927 - val_accuracy: 0.0000e+00\n","Epoch 10/20\n","15/15 [==============================] - 82s 6s/step - loss: 0.3920 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00\n","Epoch 11/20\n","15/15 [==============================] - 80s 5s/step - loss: 0.3905 - accuracy: 0.0000e+00 - val_loss: 0.3897 - val_accuracy: 0.0000e+00\n","Epoch 12/20\n","15/15 [==============================] - 74s 5s/step - loss: 0.3897 - accuracy: 0.0000e+00 - val_loss: 0.3920 - val_accuracy: 0.0000e+00\n","Epoch 13/20\n","15/15 [==============================] - 67s 5s/step - loss: 0.3901 - accuracy: 0.0000e+00 - val_loss: 0.3904 - val_accuracy: 0.0000e+00\n","Epoch 14/20\n","15/15 [==============================] - 81s 5s/step - loss: 0.3893 - accuracy: 0.0000e+00 - val_loss: 0.3888 - val_accuracy: 0.0000e+00\n","Epoch 15/20\n","15/15 [==============================] - 74s 5s/step - loss: 0.3883 - accuracy: 0.0000e+00 - val_loss: 0.3880 - val_accuracy: 0.0000e+00\n","Epoch 16/20\n","15/15 [==============================] - 79s 5s/step - loss: 0.3874 - accuracy: 0.0000e+00 - val_loss: 0.3879 - val_accuracy: 0.0000e+00\n","Epoch 17/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.3875 - accuracy: 0.0000e+00 - val_loss: 0.3878 - val_accuracy: 0.0000e+00\n","Epoch 18/20\n","15/15 [==============================] - 76s 5s/step - loss: 0.3871 - accuracy: 0.0000e+00 - val_loss: 0.3878 - val_accuracy: 0.0000e+00\n","Epoch 19/20\n","15/15 [==============================] - 69s 5s/step - loss: 0.3871 - accuracy: 0.0000e+00 - val_loss: 0.3878 - val_accuracy: 0.0000e+00\n","Epoch 20/20\n","15/15 [==============================] - 80s 5s/step - loss: 0.3867 - accuracy: 0.0000e+00 - val_loss: 0.3874 - val_accuracy: 0.0000e+00\n","4/4 [==============================] - 5s 1s/step - loss: 0.3874 - accuracy: 0.0000e+00\n","Validation Loss: 0.3874102830886841\n","Validation Accuracy: 0.0\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.applications import ResNet50\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","\n","\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0\n","    return img_array\n","\n","\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)\n","\n","\n","def create_cnn_model(input_shape, num_classes):\n","    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n","    base_model.trainable = False\n","\n","    model = tf.keras.Sequential([\n","        base_model,\n","        tf.keras.layers.GlobalAveragePooling2D(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(num_classes, activation='sigmoid')\n","    ])\n","\n","    return model\n","\n","input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n","model = create_cnn_model(input_shape, NUM_CLASSES)\n","\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","\n","history = model.fit(train_generator, epochs=EPOCHS, validation_data=(X_val, y_val))\n","\n","\n","val_loss, val_accuracy = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Accuracy: {val_accuracy}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4gzwGehwMls","executionInfo":{"status":"ok","timestamp":1717249886657,"user_tz":-330,"elapsed":2829726,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"62e43ac0-1807-4663-f150-590c6a0e7cc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 1s 0us/step\n","Epoch 1/20\n","15/15 [==============================] - 145s 9s/step - loss: 0.4895 - accuracy: 0.0063 - val_loss: 0.3954 - val_accuracy: 0.0000e+00\n","Epoch 2/20\n","15/15 [==============================] - 124s 8s/step - loss: 0.4212 - accuracy: 0.0021 - val_loss: 0.3916 - val_accuracy: 0.0000e+00\n","Epoch 3/20\n","15/15 [==============================] - 137s 9s/step - loss: 0.4110 - accuracy: 0.0021 - val_loss: 0.3844 - val_accuracy: 0.0000e+00\n","Epoch 4/20\n","15/15 [==============================] - 138s 9s/step - loss: 0.4060 - accuracy: 0.0000e+00 - val_loss: 0.3846 - val_accuracy: 0.0000e+00\n","Epoch 5/20\n","15/15 [==============================] - 141s 10s/step - loss: 0.4018 - accuracy: 0.0000e+00 - val_loss: 0.3843 - val_accuracy: 0.0000e+00\n","Epoch 6/20\n","15/15 [==============================] - 139s 10s/step - loss: 0.3980 - accuracy: 0.0000e+00 - val_loss: 0.3825 - val_accuracy: 0.0000e+00\n","Epoch 7/20\n","15/15 [==============================] - 122s 8s/step - loss: 0.3984 - accuracy: 0.0000e+00 - val_loss: 0.3837 - val_accuracy: 0.0000e+00\n","Epoch 8/20\n","15/15 [==============================] - 140s 9s/step - loss: 0.3951 - accuracy: 0.0000e+00 - val_loss: 0.3807 - val_accuracy: 0.0000e+00\n","Epoch 9/20\n","15/15 [==============================] - 124s 8s/step - loss: 0.3947 - accuracy: 0.0000e+00 - val_loss: 0.3854 - val_accuracy: 0.0000e+00\n","Epoch 10/20\n","15/15 [==============================] - 121s 8s/step - loss: 0.3945 - accuracy: 0.0000e+00 - val_loss: 0.3822 - val_accuracy: 0.0000e+00\n","Epoch 11/20\n","15/15 [==============================] - 140s 9s/step - loss: 0.3932 - accuracy: 0.0000e+00 - val_loss: 0.3805 - val_accuracy: 0.0000e+00\n","Epoch 12/20\n","15/15 [==============================] - 141s 10s/step - loss: 0.3928 - accuracy: 0.0000e+00 - val_loss: 0.3805 - val_accuracy: 0.0000e+00\n","Epoch 13/20\n","15/15 [==============================] - 138s 9s/step - loss: 0.3919 - accuracy: 0.0000e+00 - val_loss: 0.3789 - val_accuracy: 0.0000e+00\n","Epoch 14/20\n","15/15 [==============================] - 138s 9s/step - loss: 0.3928 - accuracy: 0.0000e+00 - val_loss: 0.3800 - val_accuracy: 0.0000e+00\n","Epoch 15/20\n","15/15 [==============================] - 141s 10s/step - loss: 0.3920 - accuracy: 0.0000e+00 - val_loss: 0.3795 - val_accuracy: 0.0000e+00\n","Epoch 16/20\n","15/15 [==============================] - 141s 10s/step - loss: 0.3908 - accuracy: 0.0000e+00 - val_loss: 0.3792 - val_accuracy: 0.0000e+00\n","Epoch 17/20\n","15/15 [==============================] - 136s 9s/step - loss: 0.3900 - accuracy: 0.0000e+00 - val_loss: 0.3774 - val_accuracy: 0.0000e+00\n","Epoch 18/20\n","15/15 [==============================] - 141s 10s/step - loss: 0.3882 - accuracy: 0.0000e+00 - val_loss: 0.3780 - val_accuracy: 0.0000e+00\n","Epoch 19/20\n","15/15 [==============================] - 143s 10s/step - loss: 0.3911 - accuracy: 0.0000e+00 - val_loss: 0.3783 - val_accuracy: 0.0000e+00\n","Epoch 20/20\n","15/15 [==============================] - 143s 10s/step - loss: 0.3878 - accuracy: 0.0000e+00 - val_loss: 0.3762 - val_accuracy: 0.0000e+00\n","4/4 [==============================] - 26s 7s/step - loss: 0.3762 - accuracy: 0.0000e+00\n","Validation Loss: 0.3761676251888275\n","Validation Accuracy: 0.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ALkvcdAhwVE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2JLtFjvfwVUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p8SzHSpfwVhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":485689,"status":"ok","timestamp":1717089974880,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"u_5jZDr9ghpd","outputId":"a1b78618-817e-43d5-f4bc-fbd23a8b416e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Epoch 1/20\n","15/15 [==============================] - 23s 1s/step - loss: 0.4739 - binary_accuracy: 0.7864 - auc: 0.6684 - val_loss: 0.3920 - val_binary_accuracy: 0.8366 - val_auc: 0.7640\n","Epoch 2/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.4095 - binary_accuracy: 0.8291 - auc: 0.7405 - val_loss: 0.3829 - val_binary_accuracy: 0.8389 - val_auc: 0.7840\n","Epoch 3/20\n","15/15 [==============================] - 21s 1s/step - loss: 0.3880 - binary_accuracy: 0.8368 - auc: 0.7713 - val_loss: 0.3709 - val_binary_accuracy: 0.8423 - val_auc: 0.7982\n","Epoch 4/20\n","15/15 [==============================] - 24s 2s/step - loss: 0.3784 - binary_accuracy: 0.8399 - auc: 0.7887 - val_loss: 0.3700 - val_binary_accuracy: 0.8391 - val_auc: 0.8011\n","Epoch 5/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3732 - binary_accuracy: 0.8414 - auc: 0.7946 - val_loss: 0.3606 - val_binary_accuracy: 0.8536 - val_auc: 0.8120\n","Epoch 6/20\n","15/15 [==============================] - 19s 1s/step - loss: 0.3628 - binary_accuracy: 0.8472 - auc: 0.8085 - val_loss: 0.3616 - val_binary_accuracy: 0.8459 - val_auc: 0.8114\n","Epoch 7/20\n","15/15 [==============================] - 21s 1s/step - loss: 0.3634 - binary_accuracy: 0.8465 - auc: 0.8096 - val_loss: 0.3537 - val_binary_accuracy: 0.8563 - val_auc: 0.8209\n","Epoch 8/20\n","15/15 [==============================] - 23s 2s/step - loss: 0.3534 - binary_accuracy: 0.8509 - auc: 0.8212 - val_loss: 0.3458 - val_binary_accuracy: 0.8609 - val_auc: 0.8297\n","Epoch 9/20\n","15/15 [==============================] - 19s 1s/step - loss: 0.3524 - binary_accuracy: 0.8507 - auc: 0.8245 - val_loss: 0.3553 - val_binary_accuracy: 0.8578 - val_auc: 0.8195\n","Epoch 10/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3483 - binary_accuracy: 0.8534 - auc: 0.8290 - val_loss: 0.3418 - val_binary_accuracy: 0.8617 - val_auc: 0.8331\n","Epoch 11/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3422 - binary_accuracy: 0.8551 - auc: 0.8351 - val_loss: 0.3373 - val_binary_accuracy: 0.8622 - val_auc: 0.8392\n","Epoch 12/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3379 - binary_accuracy: 0.8588 - auc: 0.8402 - val_loss: 0.3353 - val_binary_accuracy: 0.8607 - val_auc: 0.8404\n","Epoch 13/20\n","15/15 [==============================] - 19s 1s/step - loss: 0.3330 - binary_accuracy: 0.8580 - auc: 0.8472 - val_loss: 0.3399 - val_binary_accuracy: 0.8607 - val_auc: 0.8393\n","Epoch 14/20\n","15/15 [==============================] - 23s 1s/step - loss: 0.3280 - binary_accuracy: 0.8597 - auc: 0.8533 - val_loss: 0.3281 - val_binary_accuracy: 0.8641 - val_auc: 0.8495\n","Epoch 15/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3290 - binary_accuracy: 0.8625 - auc: 0.8503 - val_loss: 0.3335 - val_binary_accuracy: 0.8639 - val_auc: 0.8479\n","Epoch 16/20\n","15/15 [==============================] - 19s 1s/step - loss: 0.3237 - binary_accuracy: 0.8630 - auc: 0.8567 - val_loss: 0.3345 - val_binary_accuracy: 0.8639 - val_auc: 0.8455\n","Epoch 17/20\n","15/15 [==============================] - 22s 1s/step - loss: 0.3191 - binary_accuracy: 0.8645 - auc: 0.8614 - val_loss: 0.3227 - val_binary_accuracy: 0.8662 - val_auc: 0.8560\n","Epoch 18/20\n","15/15 [==============================] - 20s 1s/step - loss: 0.3177 - binary_accuracy: 0.8651 - auc: 0.8630 - val_loss: 0.3256 - val_binary_accuracy: 0.8656 - val_auc: 0.8540\n","Epoch 19/20\n","15/15 [==============================] - 19s 1s/step - loss: 0.3149 - binary_accuracy: 0.8651 - auc: 0.8661 - val_loss: 0.3288 - val_binary_accuracy: 0.8624 - val_auc: 0.8524\n","Epoch 20/20\n","15/15 [==============================] - 22s 1s/step - loss: 0.3154 - binary_accuracy: 0.8662 - auc: 0.8655 - val_loss: 0.3259 - val_binary_accuracy: 0.8656 - val_auc: 0.8549\n","4/4 [==============================] - 1s 220ms/step - loss: 0.3259 - binary_accuracy: 0.8656 - auc: 0.8549\n","Validation Loss: 0.32594817876815796\n","Validation Binary Accuracy: 0.865646243095398\n","Validation AUC: 0.8549269437789917\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","IMG_HEIGHT = 128\n","IMG_WIDTH = 128\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","\n","\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","\n","\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0\n","    return img_array\n","\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","\n","def create_cnn_model(input_shape, num_classes):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D((2, 2)),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(num_classes, activation='sigmoid')  # Using sigmoid for multi-label classification\n","    ])\n","    return model\n","\n","input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n","model = create_cnn_model(input_shape, NUM_CLASSES)\n","\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy', tf.keras.metrics.AUC(name='auc')])\n","\n","\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n","                    epochs=EPOCHS,\n","                    validation_data=(X_val, y_val))\n","\n","\n","val_loss, val_binary_accuracy, val_auc = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Binary Accuracy: {val_binary_accuracy}')\n","print(f'Validation AUC: {val_auc}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110497,"status":"ok","timestamp":1717090238382,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"8UzPXM77gh3L","outputId":"c21980c4-130c-43de-cd08-b817687f967a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n","Epoch 1/10\n","15/15 [==============================] - 13s 575ms/step - loss: 0.5237 - binary_accuracy: 0.7841 - auc: 0.6823 - val_loss: 0.3824 - val_binary_accuracy: 0.8491 - val_auc: 0.7810\n","Epoch 2/10\n","15/15 [==============================] - 10s 660ms/step - loss: 0.4118 - binary_accuracy: 0.8307 - auc: 0.7519 - val_loss: 0.3527 - val_binary_accuracy: 0.8624 - val_auc: 0.8144\n","Epoch 3/10\n","15/15 [==============================] - 9s 589ms/step - loss: 0.3697 - binary_accuracy: 0.8510 - auc: 0.7994 - val_loss: 0.3445 - val_binary_accuracy: 0.8663 - val_auc: 0.8239\n","Epoch 4/10\n","15/15 [==============================] - 9s 544ms/step - loss: 0.3491 - binary_accuracy: 0.8593 - auc: 0.8236 - val_loss: 0.3344 - val_binary_accuracy: 0.8682 - val_auc: 0.8372\n","Epoch 5/10\n","15/15 [==============================] - 10s 675ms/step - loss: 0.3343 - binary_accuracy: 0.8648 - auc: 0.8409 - val_loss: 0.3307 - val_binary_accuracy: 0.8690 - val_auc: 0.8424\n","Epoch 6/10\n","15/15 [==============================] - 7s 475ms/step - loss: 0.3223 - binary_accuracy: 0.8684 - auc: 0.8550 - val_loss: 0.3269 - val_binary_accuracy: 0.8716 - val_auc: 0.8489\n","Epoch 7/10\n","15/15 [==============================] - 9s 609ms/step - loss: 0.3103 - binary_accuracy: 0.8742 - auc: 0.8673 - val_loss: 0.3242 - val_binary_accuracy: 0.8723 - val_auc: 0.8515\n","Epoch 8/10\n","15/15 [==============================] - 10s 663ms/step - loss: 0.3045 - binary_accuracy: 0.8731 - auc: 0.8742 - val_loss: 0.3172 - val_binary_accuracy: 0.8735 - val_auc: 0.8575\n","Epoch 9/10\n","15/15 [==============================] - 7s 485ms/step - loss: 0.3011 - binary_accuracy: 0.8761 - auc: 0.8781 - val_loss: 0.3182 - val_binary_accuracy: 0.8719 - val_auc: 0.8564\n","Epoch 10/10\n","15/15 [==============================] - 9s 594ms/step - loss: 0.2875 - binary_accuracy: 0.8814 - auc: 0.8908 - val_loss: 0.3189 - val_binary_accuracy: 0.8716 - val_auc: 0.8572\n","4/4 [==============================] - 1s 250ms/step - loss: 0.3189 - binary_accuracy: 0.8716 - auc: 0.8572\n","Validation Loss: 0.31886065006256104\n","Validation Binary Accuracy: 0.8715986609458923\n","Validation AUC: 0.8571666479110718\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Constants\n","IMG_HEIGHT = 128\n","IMG_WIDTH = 128\n","NUM_CLASSES = 49  # 50 columns - 1 for image name\n","BATCH_SIZE = 32\n","EPOCHS = 10\n","\n","# Paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'  # Adjust this path\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load train.txt\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","\n","# Extract image names and labels\n","image_names = train_df.iloc[:, 0].astype(str).values  # Convert image names to strings\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Function to load and preprocess images\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")  # Add .jpg extension to image name\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0  # Normalize pixel values\n","    return img_array\n","\n","# Prepare dataset\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","# Load the MobileNetV2 model pre-trained on ImageNet, excluding the top layer\n","base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n","                                               include_top=False,\n","                                               weights='imagenet')\n","base_model.trainable = False  # Freeze the base model\n","\n","# Add custom layers on top of the base model\n","model = tf.keras.Sequential([\n","    base_model,\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')  # Multi-label classification\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy', tf.keras.metrics.AUC(name='auc')])\n","\n","# Add Image Data Augmentation\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# Fit the model\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n","                    epochs=EPOCHS,\n","                    validation_data=(X_val, y_val))\n","\n","# Evaluate the model\n","val_loss, val_binary_accuracy, val_auc = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Binary Accuracy: {val_binary_accuracy}')\n","print(f'Validation AUC: {val_auc}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"P9wDhg8-gh6T","outputId":"f0f3b9ec-65cf-4710-af80-0ce8e0821152"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 1s 0us/step\n","Epoch 1/20\n","15/15 [==============================] - 141s 9s/step - loss: 0.5085 - binary_accuracy: 0.7718 - auc: 0.6534 - val_loss: 0.3956 - val_binary_accuracy: 0.8376 - val_auc: 0.7671\n","Epoch 2/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.4259 - binary_accuracy: 0.8256 - auc: 0.7188 - val_loss: 0.3898 - val_binary_accuracy: 0.8362 - val_auc: 0.7673\n","Epoch 3/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.4149 - binary_accuracy: 0.8264 - auc: 0.7278 - val_loss: 0.3850 - val_binary_accuracy: 0.8371 - val_auc: 0.7710\n","Epoch 4/20\n","15/15 [==============================] - 112s 7s/step - loss: 0.4053 - binary_accuracy: 0.8320 - auc: 0.7432 - val_loss: 0.3855 - val_binary_accuracy: 0.8381 - val_auc: 0.7754\n","Epoch 5/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.4031 - binary_accuracy: 0.8318 - auc: 0.7457 - val_loss: 0.3848 - val_binary_accuracy: 0.8400 - val_auc: 0.7746\n","Epoch 6/20\n","15/15 [==============================] - 132s 9s/step - loss: 0.3997 - binary_accuracy: 0.8315 - auc: 0.7523 - val_loss: 0.3840 - val_binary_accuracy: 0.8371 - val_auc: 0.7761\n","Epoch 7/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.3990 - binary_accuracy: 0.8321 - auc: 0.7524 - val_loss: 0.3840 - val_binary_accuracy: 0.8403 - val_auc: 0.7779\n","Epoch 8/20\n","15/15 [==============================] - 133s 9s/step - loss: 0.3965 - binary_accuracy: 0.8345 - auc: 0.7552 - val_loss: 0.3831 - val_binary_accuracy: 0.8391 - val_auc: 0.7767\n","Epoch 9/20\n","15/15 [==============================] - 134s 9s/step - loss: 0.3962 - binary_accuracy: 0.8348 - auc: 0.7565 - val_loss: 0.3839 - val_binary_accuracy: 0.8395 - val_auc: 0.7776\n","Epoch 10/20\n","15/15 [==============================] - 114s 8s/step - loss: 0.3966 - binary_accuracy: 0.8363 - auc: 0.7547 - val_loss: 0.3831 - val_binary_accuracy: 0.8371 - val_auc: 0.7774\n","Epoch 11/20\n","15/15 [==============================] - 115s 8s/step - loss: 0.3943 - binary_accuracy: 0.8372 - auc: 0.7582 - val_loss: 0.3824 - val_binary_accuracy: 0.8423 - val_auc: 0.7793\n","Epoch 12/20\n","15/15 [==============================] - 112s 7s/step - loss: 0.3939 - binary_accuracy: 0.8358 - auc: 0.7594 - val_loss: 0.3816 - val_binary_accuracy: 0.8417 - val_auc: 0.7826\n","Epoch 13/20\n","15/15 [==============================] - 117s 8s/step - loss: 0.3948 - binary_accuracy: 0.8370 - auc: 0.7580 - val_loss: 0.3799 - val_binary_accuracy: 0.8410 - val_auc: 0.7822\n","Epoch 14/20\n","15/15 [==============================] - 131s 9s/step - loss: 0.3926 - binary_accuracy: 0.8366 - auc: 0.7612 - val_loss: 0.3802 - val_binary_accuracy: 0.8398 - val_auc: 0.7804\n","Epoch 15/20\n","15/15 [==============================] - 115s 8s/step - loss: 0.3900 - binary_accuracy: 0.8376 - auc: 0.7659 - val_loss: 0.3803 - val_binary_accuracy: 0.8403 - val_auc: 0.7803\n","Epoch 16/20\n","15/15 [==============================] - 110s 7s/step - loss: 0.3922 - binary_accuracy: 0.8384 - auc: 0.7614 - val_loss: 0.3792 - val_binary_accuracy: 0.8403 - val_auc: 0.7823\n","Epoch 17/20\n","15/15 [==============================] - 113s 8s/step - loss: 0.3879 - binary_accuracy: 0.8396 - auc: 0.7685 - val_loss: 0.3785 - val_binary_accuracy: 0.8423 - val_auc: 0.7847\n","Epoch 18/20\n","15/15 [==============================] - 131s 9s/step - loss: 0.3883 - binary_accuracy: 0.8375 - auc: 0.7688 - val_loss: 0.3789 - val_binary_accuracy: 0.8400 - val_auc: 0.7845\n","Epoch 19/20\n","15/15 [==============================] - 137s 9s/step - loss: 0.3890 - binary_accuracy: 0.8375 - auc: 0.7684 - val_loss: 0.3770 - val_binary_accuracy: 0.8418 - val_auc: 0.7862\n","Epoch 20/20\n","15/15 [==============================] - 132s 9s/step - loss: 0.3877 - binary_accuracy: 0.8386 - auc: 0.7682 - val_loss: 0.3781 - val_binary_accuracy: 0.8398 - val_auc: 0.7855\n","4/4 [==============================] - 20s 5s/step - loss: 0.3781 - binary_accuracy: 0.8398 - auc: 0.7855\n","Validation Loss: 0.3780742287635803\n","Validation Binary Accuracy: 0.8397958874702454\n","Validation AUC: 0.7854928970336914\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Constants\n","IMG_HEIGHT = 224  # ResNet50 input shape\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 20\n","\n","# Paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'  # Adjust this path\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load train.txt\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","\n","# Extract image names and labels\n","image_names = train_df.iloc[:, 0].astype(str).values  # Convert image names to strings\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Function to load and preprocess images\n","def load_and_preprocess_image(img_name):\n","    img_path = os.path.join(images_folder, f\"{img_name}.jpg\")  # Add .jpg extension to image name\n","    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n","    img_array = img_to_array(img)\n","    img_array = img_array / 255.0  # Normalize pixel values\n","    return img_array\n","\n","# Prepare dataset\n","images = [load_and_preprocess_image(img_name) for img_name in image_names]\n","images = np.array(images)\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","\n","# Load the ResNet50 model pre-trained on ImageNet\n","base_model = tf.keras.applications.ResNet50(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n","                                             include_top=False,\n","                                             weights='imagenet')\n","base_model.trainable = False  # Freeze the base model\n","\n","# Add custom layers on top of the base model\n","model = tf.keras.Sequential([\n","    base_model,\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')  # Multi-label classification\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy', tf.keras.metrics.AUC(name='auc')])\n","\n","# Add Image Data Augmentation\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","# Fit the model\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n","                    epochs=EPOCHS,\n","                    validation_data=(X_val, y_val))\n","\n","# Evaluate the model\n","val_loss, val_binary_accuracy, val_auc = model.evaluate(X_val, y_val)\n","print(f'Validation Loss: {val_loss}')\n","print(f'Validation Binary Accuracy: {val_binary_accuracy}')\n","print(f'Validation AUC: {val_auc}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3rnYJG0n9d9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKs81YVEn9qs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ6v_DR2n9t8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wCGrnmH5n9xB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Zu_N73An90j"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGTDQ9gVd0g47cTogRlD+1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}