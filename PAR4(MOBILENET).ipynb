{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHJyWKtEu3w1IgeLNEdI1F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUCCut5yLv0S","executionInfo":{"status":"ok","timestamp":1717882859482,"user_tz":-330,"elapsed":3149905,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"c4c7b6f0-de43-494b-b202-9bc400d57e02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100, Loss: 0.4793\n","Epoch 2/100, Loss: 0.3818\n","Epoch 3/100, Loss: 0.3519\n","Epoch 4/100, Loss: 0.3300\n","Epoch 5/100, Loss: 0.3070\n","Epoch 6/100, Loss: 0.2766\n","Epoch 7/100, Loss: 0.2418\n","Epoch 8/100, Loss: 0.2076\n","Epoch 9/100, Loss: 0.1767\n","Epoch 10/100, Loss: 0.1501\n","Epoch 11/100, Loss: 0.1287\n","Epoch 12/100, Loss: 0.1149\n","Epoch 13/100, Loss: 0.0974\n","Epoch 14/100, Loss: 0.0908\n","Epoch 15/100, Loss: 0.0832\n","Epoch 16/100, Loss: 0.0791\n","Epoch 17/100, Loss: 0.0817\n","Epoch 18/100, Loss: 0.0696\n","Epoch 19/100, Loss: 0.0662\n","Epoch 20/100, Loss: 0.0621\n","Epoch 21/100, Loss: 0.0633\n","Epoch 22/100, Loss: 0.0557\n","Epoch 23/100, Loss: 0.0529\n","Epoch 24/100, Loss: 0.0550\n","Epoch 25/100, Loss: 0.0513\n","Epoch 26/100, Loss: 0.0488\n","Epoch 27/100, Loss: 0.0484\n","Epoch 28/100, Loss: 0.0510\n","Epoch 29/100, Loss: 0.0465\n","Epoch 30/100, Loss: 0.0447\n","Epoch 31/100, Loss: 0.0468\n","Epoch 32/100, Loss: 0.0429\n","Epoch 33/100, Loss: 0.0400\n","Epoch 34/100, Loss: 0.0445\n","Epoch 35/100, Loss: 0.0414\n","Epoch 36/100, Loss: 0.0404\n","Epoch 37/100, Loss: 0.0378\n","Epoch 38/100, Loss: 0.0420\n","Epoch 39/100, Loss: 0.0354\n","Epoch 40/100, Loss: 0.0375\n","Epoch 41/100, Loss: 0.0374\n","Epoch 42/100, Loss: 0.0414\n","Epoch 43/100, Loss: 0.0357\n","Epoch 44/100, Loss: 0.0332\n","Epoch 45/100, Loss: 0.0334\n","Epoch 46/100, Loss: 0.0354\n","Epoch 47/100, Loss: 0.0346\n","Epoch 48/100, Loss: 0.0328\n","Epoch 49/100, Loss: 0.0339\n","Epoch 50/100, Loss: 0.0339\n","Epoch 51/100, Loss: 0.0325\n","Epoch 52/100, Loss: 0.0336\n","Epoch 53/100, Loss: 0.0324\n","Epoch 54/100, Loss: 0.0279\n","Epoch 55/100, Loss: 0.0340\n","Epoch 56/100, Loss: 0.0307\n","Epoch 57/100, Loss: 0.0337\n","Epoch 58/100, Loss: 0.0332\n","Epoch 59/100, Loss: 0.0280\n","Epoch 60/100, Loss: 0.0281\n","Epoch 61/100, Loss: 0.0286\n","Epoch 62/100, Loss: 0.0270\n","Epoch 63/100, Loss: 0.0318\n","Epoch 64/100, Loss: 0.0286\n","Epoch 65/100, Loss: 0.0305\n","Epoch 66/100, Loss: 0.0303\n","Epoch 67/100, Loss: 0.0303\n","Epoch 68/100, Loss: 0.0268\n","Epoch 69/100, Loss: 0.0285\n","Epoch 70/100, Loss: 0.0274\n","Epoch 71/100, Loss: 0.0276\n","Epoch 72/100, Loss: 0.0267\n","Epoch 73/100, Loss: 0.0285\n","Epoch 74/100, Loss: 0.0270\n","Epoch 75/100, Loss: 0.0293\n","Epoch 76/100, Loss: 0.0263\n","Epoch 77/100, Loss: 0.0248\n","Epoch 78/100, Loss: 0.0267\n","Epoch 79/100, Loss: 0.0275\n","Epoch 80/100, Loss: 0.0263\n","Epoch 81/100, Loss: 0.0280\n","Epoch 82/100, Loss: 0.0275\n","Epoch 83/100, Loss: 0.0242\n","Epoch 84/100, Loss: 0.0254\n","Epoch 85/100, Loss: 0.0263\n","Epoch 86/100, Loss: 0.0239\n","Epoch 87/100, Loss: 0.0234\n","Epoch 88/100, Loss: 0.0241\n","Epoch 89/100, Loss: 0.0239\n","Epoch 90/100, Loss: 0.0242\n","Epoch 91/100, Loss: 0.0259\n","Epoch 92/100, Loss: 0.0253\n","Epoch 93/100, Loss: 0.0216\n","Epoch 94/100, Loss: 0.0236\n","Epoch 95/100, Loss: 0.0246\n","Epoch 96/100, Loss: 0.0202\n","Epoch 97/100, Loss: 0.0231\n","Epoch 98/100, Loss: 0.0216\n","Epoch 99/100, Loss: 0.0245\n","Epoch 100/100, Loss: 0.0230\n","Model saved to simple_cnn_model.pth\n","Validation Loss: 0.5282\n","Validation Accuracy: 0.8811\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 100\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the model\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128 * 28 * 28, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# Instantiate and compile the model\n","model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Save the trained model\n","model_path = \"simple_cnn_model.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n","\n","# Validation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / (len(val_loader.dataset) * NUM_CLASSES)\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n","\n","# # Load the model for inference\n","# loaded_model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n","# loaded_model.load_state_dict(torch.load(model_path))\n","# loaded_model.eval()\n","\n","# Function to make predictions on test data\n","# def predict(model, image_path):\n","#     transform = transforms.Compose([\n","#         transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","#         transforms.ToTensor(),\n","#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","#     ])\n","#     image = Image.open(image_path).convert(\"RGB\")\n","#     image = transform(image).unsqueeze(0).to(DEVICE)\n","#     with torch.no_grad():\n","#         output = model(image)\n","#     return output.cpu().numpy()\n","\n","# # Example usage\n","# test_image_path = os.path.join(images_folder, \"test_image.jpg\")\n","# prediction = predict(loaded_model, test_image_path)\n","# print(f\"Prediction: {prediction}\")\n"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","test_images_folder = os.path.join(drive_base_path, 'test_images')  # Folder containing test images\n","\n","# List all test image files\n","test_image_names = [f.split('.')[0] for f in os.listdir(test_images_folder) if f.endswith('.jpg')]\n","\n","# Custom Dataset for Test Data\n","class TestDataset(Dataset):\n","    def __init__(self, image_names, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, img_name\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader for test data\n","test_dataset = TestDataset(test_image_names, test_images_folder, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the model (same as before)\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128 * 28 * 28, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# Load the model\n","model_path = \"simple_cnn_model.pth\"\n","model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","# Function to make predictions on test data\n","def predict(model, dataloader):\n","    model.eval()\n","    predictions = {}\n","    with torch.no_grad():\n","        for inputs, img_names in dataloader:\n","            inputs = inputs.to(DEVICE)\n","            outputs = model(inputs)\n","            outputs = outputs.cpu().numpy()\n","            for img_name, output in zip(img_names, outputs):\n","                predictions[img_name] = output\n","    return predictions\n","\n","# Make predictions on the test dataset\n","predictions = predict(model, test_loader)\n","\n","# Example: Print the predictions for the first few test images\n","for img_name, output in list(predictions.items())[:5]:\n","    print(f\"Image: {img_name}, Prediction: {output}\")\n","\n","# Save predictions to a CSV file\n","predictions_df = pd.DataFrame.from_dict(predictions, orient='index')\n","predictions_df.to_csv('predictions.csv', header=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMkedQwWNeBz","executionInfo":{"status":"ok","timestamp":1717882912865,"user_tz":-330,"elapsed":15711,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"74e94b79-022b-4949-fe95-a5da11f96b85"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Image: 144, Prediction: [1.3969039e-09 5.6472782e-09 1.3411777e-02 3.5671253e-06 1.5681745e-06\n"," 1.0906715e-02 3.0135669e-05 8.3145094e-01 3.9098426e-04 5.9748261e-04\n"," 2.5925738e-06 7.5860029e-01 6.5264367e-06 2.1310354e-05 7.8810444e-05\n"," 2.5033567e-04 1.3196809e-04 2.8477572e-03 2.2091040e-08 3.7270127e-04\n"," 5.3147909e-05 6.8726023e-03 1.4687663e-02 4.5408669e-04 8.7678543e-04\n"," 2.6945725e-06 7.6059740e-07 2.3910549e-02 4.2543764e-08 6.5023892e-02\n"," 2.1795513e-05 3.7720434e-02 6.3160801e-06 4.4323850e-02 4.3876025e-01\n"," 6.8218127e-04 1.4952344e-03 4.1276618e-07 8.8219261e-01 1.0212062e-02\n"," 3.5972789e-05 9.1548462e-04 5.1822895e-01 1.5949301e-04 4.4522469e-05\n"," 9.9966395e-01 7.2351009e-05 9.9943620e-01 1.8247033e-03]\n","Image: 24, Prediction: [1.2388267e-04 5.0449162e-06 6.8719138e-04 3.4082568e-01 8.3987676e-02\n"," 1.5452424e-05 2.1961877e-02 8.2681058e-03 2.3885326e-04 3.4537405e-02\n"," 3.1676758e-05 4.2362455e-03 7.4313857e-02 4.3675154e-03 1.5410262e-05\n"," 2.1310790e-01 1.6526175e-03 1.1134360e-03 4.0148621e-04 2.2914607e-02\n"," 1.6513250e-04 3.3769693e-02 7.2719433e-07 2.1538917e-02 3.9378996e-03\n"," 2.2897361e-02 2.1147230e-03 7.5230104e-01 2.6514186e-07 2.5747111e-02\n"," 2.5275795e-04 2.9423606e-02 1.2729963e-04 2.4642199e-01 2.1059300e-01\n"," 3.4329826e-03 6.7078887e-04 7.3223899e-05 1.0319217e-01 3.2881695e-01\n"," 5.8860064e-04 1.8842714e-02 8.9913303e-01 3.4545737e-03 2.9805899e-05\n"," 9.9526972e-01 9.9834561e-01 2.0755788e-03 1.9761812e-05]\n","Image: 124, Prediction: [2.7873074e-03 4.3523218e-05 8.6711795e-04 3.8894685e-04 9.5612626e-04\n"," 2.1613181e-01 3.1185718e-03 4.4187834e-03 4.4203121e-02 1.3821469e-02\n"," 5.5434793e-01 2.4352390e-03 4.8987467e-06 4.1509438e-03 2.5797519e-03\n"," 8.0164718e-05 2.1421757e-04 7.6941862e-05 9.2227105e-04 2.0158652e-02\n"," 5.7718591e-03 1.6814456e-04 8.0288236e-04 2.0365433e-03 2.6527771e-01\n"," 3.0140814e-04 6.7548743e-03 1.0543413e-03 1.1282177e-03 2.6089114e-03\n"," 6.7513984e-01 3.7439799e-04 7.4188580e-04 2.9996142e-01 1.7079521e-02\n"," 2.1713765e-02 3.7901929e-01 1.5384334e-02 8.2243365e-05 1.3413627e-02\n"," 5.7334941e-02 3.8265422e-01 4.9706907e-03 3.1791165e-04 4.9681979e-04\n"," 9.9222022e-01 9.3507461e-02 4.0438592e-01 9.7318115e-03]\n","Image: 66, Prediction: [5.48005664e-05 5.15898690e-04 1.61554726e-05 1.25679485e-02\n"," 8.42572331e-01 2.77019222e-03 1.85058045e-04 2.57158224e-02\n"," 3.39366816e-04 1.12301700e-01 2.84154899e-02 6.84303558e-03\n"," 1.39824942e-05 6.42975374e-06 7.30004744e-04 1.54188470e-04\n"," 8.23073206e-05 6.96243998e-03 3.37522780e-03 8.14597867e-03\n"," 2.43816455e-03 1.55039446e-03 5.30520314e-03 2.49767581e-05\n"," 2.71491408e-01 1.64982368e-04 1.16438474e-04 4.37396084e-04\n"," 7.18640047e-04 2.92313825e-02 3.23871911e-01 1.08148158e-02\n"," 1.11477845e-03 6.18170500e-01 1.12317815e-01 5.42498485e-04\n"," 2.27374025e-02 5.61925757e-04 1.11856814e-02 1.31165326e-01\n"," 8.73343088e-04 4.37294960e-01 1.08381182e-01 4.42027301e-03\n"," 2.81354325e-04 9.98025298e-01 8.70085359e-01 1.72352083e-02\n"," 1.14839722e-03]\n","Image: 107, Prediction: [3.7650125e-06 9.3863910e-04 5.7085656e-04 9.2069131e-06 4.9568133e-08\n"," 3.0502045e-01 1.8664867e-04 1.6354266e-04 5.9341959e-04 9.9869627e-01\n"," 4.5444693e-05 1.6366446e-06 9.9576489e-09 5.5610005e-08 9.5341427e-05\n"," 2.0898765e-06 1.9029190e-04 8.5535123e-10 8.4146382e-07 8.7277776e-09\n"," 1.9106454e-09 2.0862974e-09 9.4235843e-01 2.4205592e-08 8.7867177e-04\n"," 5.8028374e-05 8.5015699e-08 1.8334445e-07 3.0057570e-05 9.9833339e-01\n"," 1.4364424e-05 9.4115782e-09 8.9691748e-06 9.9860966e-01 7.8602573e-03\n"," 1.2401775e-10 5.7419711e-06 7.0832866e-05 5.2240665e-07 9.4449073e-01\n"," 2.2472447e-04 1.5853147e-01 1.5796522e-02 9.9999869e-01 1.3600755e-06\n"," 9.7256634e-07 6.7004757e-03 9.7277021e-01 1.1611482e-04]\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","test_images_folder = os.path.join(drive_base_path, 'test_images')  # Folder containing test images\n","\n","# List all test image files\n","test_image_names = [f.split('.')[0] for f in os.listdir(test_images_folder) if f.endswith('.jpg')]\n","\n","# Custom Dataset for Test Data\n","class TestDataset(Dataset):\n","    def __init__(self, image_names, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, img_name\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader for test data\n","test_dataset = TestDataset(test_image_names, test_images_folder, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the model (same as before)\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(128 * 28 * 28, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.classifier(x)\n","        return x\n","\n","# Load the model\n","model_path = \"simple_cnn_model.pth\"\n","model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n","model.load_state_dict(torch.load(model_path))\n","model.eval()\n","\n","# Function to make predictions on test data\n","def predict(model, dataloader, threshold=0.5):\n","    model.eval()\n","    predictions = {}\n","    with torch.no_grad():\n","        for inputs, img_names in dataloader:\n","            inputs = inputs.to(DEVICE)\n","            outputs = model(inputs)\n","            outputs = outputs.cpu().numpy()\n","            binary_outputs = (outputs >= threshold).astype(int)\n","            for img_name, output in zip(img_names, binary_outputs):\n","                predictions[img_name] = output\n","    return predictions\n","\n","# Make predictions on the test dataset\n","predictions = predict(model, test_loader)\n","\n","# Example: Print the predictions for the first few test images\n","for img_name, output in list(predictions.items())[:5]:\n","    print(f\"Image: {img_name}, Prediction: {output}\")\n","\n","# Save predictions to a CSV file\n","predictions_df = pd.DataFrame.from_dict(predictions, orient='index')\n","predictions_df.to_csv('binary_predictions.csv', header=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SORzkS5oPezf","executionInfo":{"status":"ok","timestamp":1717882945941,"user_tz":-330,"elapsed":15962,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"53bf4c37-1acc-4af5-ba69-85a4149874da"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Image: 144, Prediction: [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 1 0 0 0 1 0 0 1 0 1 0]\n","Image: 24, Prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 1 0 0 1 1 0 0]\n","Image: 124, Prediction: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 1 0 0 0]\n","Image: 66, Prediction: [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n"," 0 0 0 0 0 0 0 0 1 1 0 0]\n","Image: 107, Prediction: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n"," 0 0 1 0 0 0 1 0 0 0 1 0]\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s16A3EcfL8rN","executionInfo":{"status":"ok","timestamp":1717872653683,"user_tz":-330,"elapsed":33590,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"7a01aa74-8e76-4755-806a-181f20ec3d03"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}