{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20043,"status":"ok","timestamp":1717430381628,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"Tg-6npojDOVo","outputId":"471b72fa-ec45-42c1-de2d-127157f563c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1644,"status":"ok","timestamp":1717430391026,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"},"user_tz":-330},"id":"qhI4JhvUDAHc","outputId":"c1452267-d4c1-45cd-d4b0-db04d659f891"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files extracted to: /content/\n"]},{"output_type":"execute_result","data":{"text/plain":["['.config', 'drive', 'VRL_challenge_PAR', 'sample_data']"]},"metadata":{},"execution_count":3}],"source":["import zipfile\n","import os\n","\n","\n","zip_file_name = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR.zip'\n","\n","\n","extract_to = '/content/'\n","\n","with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to)\n","print(\"Files extracted to:\", extract_to)\n","os.listdir(extract_to)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKoCz_-fC52I","outputId":"370dba47-b432-4ed7-ab1c-e29672211e0e"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 86.2MB/s]\n","<ipython-input-4-7fe9f971f46d>:130: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  all_labels = torch.tensor(all_labels)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/200], Train Loss: 0.5511, Val Loss: 0.4683, Val Acc: 0.0000, Val F1: 0.1448\n","Epoch [2/200], Train Loss: 0.3641, Val Loss: 0.4439, Val Acc: 0.0000, Val F1: 0.1745\n","Epoch [3/200], Train Loss: 0.3109, Val Loss: 0.4279, Val Acc: 0.0000, Val F1: 0.2252\n","Epoch [4/200], Train Loss: 0.2814, Val Loss: 0.4115, Val Acc: 0.0000, Val F1: 0.2483\n","Epoch [5/200], Train Loss: 0.2555, Val Loss: 0.3985, Val Acc: 0.0000, Val F1: 0.2782\n","Epoch [6/200], Train Loss: 0.2328, Val Loss: 0.3964, Val Acc: 0.0000, Val F1: 0.3029\n","Epoch [7/200], Train Loss: 0.2116, Val Loss: 0.4047, Val Acc: 0.0000, Val F1: 0.3169\n","Epoch [8/200], Train Loss: 0.1949, Val Loss: 0.3942, Val Acc: 0.0000, Val F1: 0.3239\n","Epoch [9/200], Train Loss: 0.1901, Val Loss: 0.3875, Val Acc: 0.0000, Val F1: 0.3350\n","Epoch [10/200], Train Loss: 0.1884, Val Loss: 0.3872, Val Acc: 0.0000, Val F1: 0.3352\n","Epoch [11/200], Train Loss: 0.1893, Val Loss: 0.3891, Val Acc: 0.0000, Val F1: 0.3311\n","Epoch [12/200], Train Loss: 0.1848, Val Loss: 0.3871, Val Acc: 0.0000, Val F1: 0.3384\n","Epoch [13/200], Train Loss: 0.1837, Val Loss: 0.3881, Val Acc: 0.0000, Val F1: 0.3432\n","Epoch [14/200], Train Loss: 0.1813, Val Loss: 0.3875, Val Acc: 0.0000, Val F1: 0.3429\n","Epoch [15/200], Train Loss: 0.1796, Val Loss: 0.3883, Val Acc: 0.0000, Val F1: 0.3463\n","Epoch [16/200], Train Loss: 0.1785, Val Loss: 0.3816, Val Acc: 0.0000, Val F1: 0.3494\n","Epoch [17/200], Train Loss: 0.1765, Val Loss: 0.3836, Val Acc: 0.0000, Val F1: 0.3446\n","Epoch [18/200], Train Loss: 0.1756, Val Loss: 0.3843, Val Acc: 0.0000, Val F1: 0.3440\n","Epoch [19/200], Train Loss: 0.1761, Val Loss: 0.3866, Val Acc: 0.0000, Val F1: 0.3436\n","Epoch [20/200], Train Loss: 0.1778, Val Loss: 0.3839, Val Acc: 0.0000, Val F1: 0.3475\n","Epoch [21/200], Train Loss: 0.1767, Val Loss: 0.3851, Val Acc: 0.0000, Val F1: 0.3434\n","Epoch [22/200], Train Loss: 0.1754, Val Loss: 0.3856, Val Acc: 0.0000, Val F1: 0.3443\n","Epoch [23/200], Train Loss: 0.1762, Val Loss: 0.3851, Val Acc: 0.0000, Val F1: 0.3432\n","Epoch [24/200], Train Loss: 0.1735, Val Loss: 0.3856, Val Acc: 0.0000, Val F1: 0.3450\n","Epoch [25/200], Train Loss: 0.1770, Val Loss: 0.3826, Val Acc: 0.0000, Val F1: 0.3455\n","Epoch [26/200], Train Loss: 0.1753, Val Loss: 0.3875, Val Acc: 0.0000, Val F1: 0.3473\n","Epoch [27/200], Train Loss: 0.1757, Val Loss: 0.3863, Val Acc: 0.0000, Val F1: 0.3425\n","Epoch [28/200], Train Loss: 0.1737, Val Loss: 0.3855, Val Acc: 0.0000, Val F1: 0.3456\n","Epoch [29/200], Train Loss: 0.1754, Val Loss: 0.3857, Val Acc: 0.0000, Val F1: 0.3481\n","Epoch [30/200], Train Loss: 0.1776, Val Loss: 0.3847, Val Acc: 0.0000, Val F1: 0.3475\n","Epoch [31/200], Train Loss: 0.1754, Val Loss: 0.3848, Val Acc: 0.0000, Val F1: 0.3426\n","Epoch [32/200], Train Loss: 0.1761, Val Loss: 0.3856, Val Acc: 0.0000, Val F1: 0.3492\n","Epoch [33/200], Train Loss: 0.1751, Val Loss: 0.3836, Val Acc: 0.0000, Val F1: 0.3471\n","Epoch [34/200], Train Loss: 0.1747, Val Loss: 0.3866, Val Acc: 0.0000, Val F1: 0.3467\n","Epoch [35/200], Train Loss: 0.1739, Val Loss: 0.3823, Val Acc: 0.0000, Val F1: 0.3507\n","Epoch [36/200], Train Loss: 0.1778, Val Loss: 0.3858, Val Acc: 0.0000, Val F1: 0.3470\n","Epoch [37/200], Train Loss: 0.1754, Val Loss: 0.3882, Val Acc: 0.0000, Val F1: 0.3455\n","Epoch [38/200], Train Loss: 0.1750, Val Loss: 0.3848, Val Acc: 0.0000, Val F1: 0.3476\n","Epoch [39/200], Train Loss: 0.1753, Val Loss: 0.3866, Val Acc: 0.0000, Val F1: 0.3474\n","Epoch [40/200], Train Loss: 0.1759, Val Loss: 0.3803, Val Acc: 0.0000, Val F1: 0.3481\n","Epoch [41/200], Train Loss: 0.1765, Val Loss: 0.3855, Val Acc: 0.0000, Val F1: 0.3480\n","Epoch [42/200], Train Loss: 0.1754, Val Loss: 0.3876, Val Acc: 0.0000, Val F1: 0.3465\n","Epoch [43/200], Train Loss: 0.1809, Val Loss: 0.3824, Val Acc: 0.0000, Val F1: 0.3480\n","Epoch [44/200], Train Loss: 0.1771, Val Loss: 0.3866, Val Acc: 0.0000, Val F1: 0.3451\n","Epoch [45/200], Train Loss: 0.1771, Val Loss: 0.3841, Val Acc: 0.0000, Val F1: 0.3449\n","Epoch [46/200], Train Loss: 0.1741, Val Loss: 0.3859, Val Acc: 0.0000, Val F1: 0.3461\n","Epoch [47/200], Train Loss: 0.1770, Val Loss: 0.3848, Val Acc: 0.0000, Val F1: 0.3477\n","Epoch [48/200], Train Loss: 0.1767, Val Loss: 0.3826, Val Acc: 0.0000, Val F1: 0.3464\n","Epoch [49/200], Train Loss: 0.1753, Val Loss: 0.3842, Val Acc: 0.0000, Val F1: 0.3456\n","Epoch [50/200], Train Loss: 0.1798, Val Loss: 0.3856, Val Acc: 0.0000, Val F1: 0.3474\n","Epoch [51/200], Train Loss: 0.1760, Val Loss: 0.3840, Val Acc: 0.0000, Val F1: 0.3449\n","Epoch [52/200], Train Loss: 0.1743, Val Loss: 0.3878, Val Acc: 0.0000, Val F1: 0.3434\n","Epoch [53/200], Train Loss: 0.1748, Val Loss: 0.3851, Val Acc: 0.0000, Val F1: 0.3447\n","Epoch [54/200], Train Loss: 0.1755, Val Loss: 0.3803, Val Acc: 0.0000, Val F1: 0.3480\n","Epoch [55/200], Train Loss: 0.1761, Val Loss: 0.3842, Val Acc: 0.0000, Val F1: 0.3472\n","Epoch [56/200], Train Loss: 0.1745, Val Loss: 0.3903, Val Acc: 0.0000, Val F1: 0.3435\n","Epoch [57/200], Train Loss: 0.1747, Val Loss: 0.3809, Val Acc: 0.0000, Val F1: 0.3441\n","Epoch [58/200], Train Loss: 0.1728, Val Loss: 0.3837, Val Acc: 0.0000, Val F1: 0.3411\n","Epoch [59/200], Train Loss: 0.1750, Val Loss: 0.3840, Val Acc: 0.0000, Val F1: 0.3459\n","Epoch [60/200], Train Loss: 0.1764, Val Loss: 0.3847, Val Acc: 0.0000, Val F1: 0.3492\n","Epoch [61/200], Train Loss: 0.1762, Val Loss: 0.3868, Val Acc: 0.0000, Val F1: 0.3407\n","Epoch [62/200], Train Loss: 0.1756, Val Loss: 0.3850, Val Acc: 0.0000, Val F1: 0.3439\n","Epoch [63/200], Train Loss: 0.1759, Val Loss: 0.3862, Val Acc: 0.0000, Val F1: 0.3453\n","Epoch [64/200], Train Loss: 0.1757, Val Loss: 0.3847, Val Acc: 0.0000, Val F1: 0.3434\n","Epoch [65/200], Train Loss: 0.1753, Val Loss: 0.3838, Val Acc: 0.0000, Val F1: 0.3435\n","Epoch [66/200], Train Loss: 0.1750, Val Loss: 0.3836, Val Acc: 0.0000, Val F1: 0.3469\n","Epoch [67/200], Train Loss: 0.1745, Val Loss: 0.3878, Val Acc: 0.0000, Val F1: 0.3457\n","Epoch [68/200], Train Loss: 0.1768, Val Loss: 0.3895, Val Acc: 0.0000, Val F1: 0.3469\n","Epoch [69/200], Train Loss: 0.1774, Val Loss: 0.3834, Val Acc: 0.0000, Val F1: 0.3450\n","Epoch [70/200], Train Loss: 0.1748, Val Loss: 0.3852, Val Acc: 0.0000, Val F1: 0.3481\n","Epoch [71/200], Train Loss: 0.1758, Val Loss: 0.3853, Val Acc: 0.0000, Val F1: 0.3427\n","Epoch [72/200], Train Loss: 0.1763, Val Loss: 0.3888, Val Acc: 0.0000, Val F1: 0.3404\n"]}],"source":["import os\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, f1_score\n","import pandas as pd\n","\n","\n","class PAR_Dataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, label_file, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations(annotation_file)\n","        self.labels = self.load_labels(label_file)\n","\n","    def load_annotations(self, annotation_file):\n","        with open(annotation_file, 'r') as f:\n","            lines = f.readlines()\n","        annotations = []\n","        for line in lines:\n","            parts = line.strip().split()\n","            image_name = parts[0] + '.jpg'\n","            attributes = list(map(int, parts[1:]))\n","            annotations.append((image_name, attributes))\n","        return annotations\n","\n","    def load_labels(self, label_file):\n","        with open(label_file, 'r') as f:\n","            labels = [line.strip() for line in f.readlines()]\n","        return labels\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name, attributes = self.annotations[idx]\n","        img_path = os.path.join(self.image_dir, image_name)\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        attributes = torch.tensor(attributes, dtype=torch.float32)\n","        return image, attributes\n","\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","class PAR_Model(nn.Module):\n","    def __init__(self, num_attributes):\n","        super(PAR_Model, self).__init__()\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_attributes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","\n","num_epochs = 200\n","learning_rate = 0.0001\n","batch_size = 16\n","\n","\n","train_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=train_transform)\n","val_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model, loss function, and optimizer\n","num_attributes = len(train_dataset.labels)\n","model = PAR_Model(num_attributes)\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)  # Learning rate scheduler\n","\n","# Training and validation loop\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, labels in train_loader:\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_outputs = []\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_outputs.extend(outputs.cpu().numpy())\n","\n","    val_loss /= len(val_loader)\n","    scheduler.step()\n","\n","    all_labels = torch.tensor(all_labels)\n","    all_outputs = torch.tensor(all_outputs)\n","    val_acc = accuracy_score(all_labels, all_outputs > 0.5)\n","    val_f1 = f1_score(all_labels, all_outputs > 0.5, average='weighted')\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'par_model_resnet50_best.pth')\n","\n","# Predict on test images and save results to CSV\n","def predict_on_images(model, image_folder, output_csv, transform):\n","    model.eval()\n","    image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(('.jpg', '.jpeg', '.png'))]\n","    image_paths.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))  # Sort by image name assuming names are like 1.jpg, 2.jpg, etc.\n","\n","    predictions = []\n","\n","    for image_path in image_paths:\n","        image = Image.open(image_path).convert('RGB')\n","        image = transform(image).unsqueeze(0)  # Add batch dimension\n","\n","        with torch.no_grad():\n","            outputs = model(image)\n","            outputs = (outputs > 0.5).int().squeeze().tolist()  # Apply threshold\n","\n","        image_name = os.path.basename(image_path)\n","        predictions.append([image_name] + outputs)\n","\n","    # Save to CSV\n","    header = ['image_name'] + [f'attribute_{i}' for i in range(1, num_attributes + 1)]\n","    df = pd.DataFrame(predictions, columns=header)\n","    df.to_csv(output_csv, index=False)\n","    print(f\"Predictions saved to {output_csv}\")\n","\n","# Path to the folder containing the test images\n","test_image_folder = '/content/drive/MyDrive/ANSYS/SCSPAR24_Testdata'  # Replace with your actual test image folder path\n","output_csv = '/content/predictions.csv'\n","\n","# Load the best model for predictions\n","model.load_state_dict(torch.load('par_model_resnet50_best.pth'))\n","\n","# Run predictions and save to CSV\n","predict_on_images(model, test_image_folder, output_csv, val_transform)\n"]},{"cell_type":"code","source":["import os\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, f1_score\n","import pandas as pd\n","\n","\n","class PAR_Dataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, label_file, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations(annotation_file)\n","        self.labels = self.load_labels(label_file)\n","        self.image_paths = {os.path.splitext(f)[0]: os.path.join(image_dir, f) for f in os.listdir(image_dir)}\n","\n","    def load_annotations(self, annotation_file):\n","        with open(annotation_file, 'r') as f:\n","            lines = f.readlines()\n","        annotations = []\n","        for line in lines:\n","            parts = line.strip().split()\n","            image_name = parts[0]\n","            attributes = list(map(int, parts[1:]))\n","            annotations.append((image_name, attributes))\n","        return annotations\n","\n","    def load_labels(self, label_file):\n","        with open(label_file, 'r') as f:\n","            labels = [line.strip() for line in f.readlines()]\n","        return labels\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name, attributes = self.annotations[idx]\n","        img_path = self.image_paths[image_name]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        attributes = torch.tensor(attributes, dtype=torch.float32)\n","        return image, attributes\n"],"metadata":{"id":"9Qf059rLhrLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"],"metadata":{"id":"yM0NXTKlhrcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PAR_Model(nn.Module):\n","    def __init__(self, num_attributes):\n","        super(PAR_Model, self).__init__()\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_attributes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.sigmoid(x)\n","        return x\n"],"metadata":{"id":"kI3nt_1fhrgs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 2\n","learning_rate = 0.0001\n","batch_size = 16\n","\n","train_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=train_transform)\n","val_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","num_attributes = len(train_dataset.labels)\n","model = PAR_Model(num_attributes)\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, labels in train_loader:\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_outputs = []\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_outputs.extend(outputs.cpu().numpy())\n","\n","    val_loss /= len(val_loader)\n","    scheduler.step()\n","\n","    all_labels = torch.tensor(all_labels)\n","    all_outputs = torch.tensor(all_outputs)\n","    val_acc = accuracy_score(all_labels, all_outputs > 0.5)\n","    val_f1 = f1_score(all_labels, all_outputs > 0.5, average='weighted')\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'par_model_resnet50_best.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94ZAGDBghrlf","executionInfo":{"status":"ok","timestamp":1717006576890,"user_tz":-330,"elapsed":600486,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"8560d28e-2c29-4ed6-e3be-3c7d6590f274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 63.0MB/s]\n","<ipython-input-6-9f1cb33dd7a9>:51: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  all_labels = torch.tensor(all_labels)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Train Loss: 0.5685, Val Loss: 0.4937, Val Acc: 0.0000, Val F1: 0.1609\n","Epoch [2/2], Train Loss: 0.3727, Val Loss: 0.4588, Val Acc: 0.0000, Val F1: 0.1761\n"]}]},{"cell_type":"code","source":["class PAR_Dataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, label_file, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations(annotation_file)\n","        self.labels = self.load_labels(label_file)\n","        self.image_paths = {os.path.splitext(f)[0]: os.path.join(image_dir, f) for f in os.listdir(image_dir)}\n","\n","    def load_annotations(self, annotation_file):\n","        with open(annotation_file, 'r') as f:\n","            lines = f.readlines()\n","        annotations = []\n","        for line in lines:\n","            parts = line.strip().split()\n","            image_name = parts[0]\n","            attributes = list(map(int, parts[1:]))\n","            annotations.append((image_name, attributes))\n","        return annotations\n","\n","    def load_labels(self, label_file):\n","        with open(label_file, 'r') as f:\n","            labels = [line.strip() for line in f.readlines()]\n","        return labels\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name, attributes = self.annotations[idx]\n","        img_path = self.image_paths[image_name]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        attributes = torch.tensor(attributes, dtype=torch.float32)\n","        return image, attributes\n"],"metadata":{"id":"FRMDm1e5hrpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","num_epochs = 1\n","learning_rate = 0.0001\n","batch_size = 16\n","\n","train_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=train_transform)\n","val_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","num_attributes = len(train_dataset.labels)\n","model = PAR_Model(num_attributes)\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, labels in train_loader:\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_outputs = []\n","\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_outputs.extend(outputs.cpu().numpy())\n","\n","    val_loss /= len(val_loader)\n","    scheduler.step()\n","\n","    all_labels = np.array(all_labels)\n","    all_outputs = np.array(all_outputs)\n","    val_acc = accuracy_score(all_labels, all_outputs > 0.1)\n","    val_f1 = f1_score(all_labels, all_outputs > 0.1, average='weighted')\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'par_model_resnet50_best.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybf0jkTyk44m","executionInfo":{"status":"ok","timestamp":1717007845562,"user_tz":-330,"elapsed":306109,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"a599d7f6-9bbc-46c7-fa5f-13560da639e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/1], Train Loss: 0.5550, Val Loss: 0.4800, Val Acc: 0.0000, Val F1: 0.1768\n"]}]},{"cell_type":"code","source":["# Debugging: Print a sample of predictions and corresponding labels\n","sample_size = 5\n","print(\"Sample predictions and labels:\")\n","for i in range(sample_size):\n","    print(f\"Prediction: {all_outputs[i]}, Label: {all_labels[i]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuzhuztWk5GB","executionInfo":{"status":"ok","timestamp":1717007376040,"user_tz":-330,"elapsed":784,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"964418c5-73f9-4fdd-f242-bb87ccd7812e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample predictions and labels:\n","Prediction: [0.09589572 0.09972013 0.08869521 0.04577098 0.05574826 0.22703566\n"," 0.0638983  0.18464483 0.08444769 0.46409252 0.14015554 0.05485197\n"," 0.0455873  0.04473143 0.07262193 0.04236887 0.0755322  0.04632498\n"," 0.0806694  0.05131893 0.0498988  0.06160627 0.28105184 0.08366938\n"," 0.18467076 0.17501098 0.07786711 0.06601954 0.03573979 0.61859727\n"," 0.12986138 0.03897052 0.08915626 0.8075033  0.15205511 0.09102902\n"," 0.17420816 0.10386563 0.07306469 0.3025067  0.05996171 0.48080343\n"," 0.23239644 0.4288802  0.05478685 0.5398211  0.60016936 0.33856764\n"," 0.10845541], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n"," 0.]\n","Prediction: [0.14021645 0.16847602 0.10646795 0.06631079 0.10488731 0.3600579\n"," 0.09023795 0.24833031 0.1233279  0.60722923 0.22356336 0.07534391\n"," 0.06988175 0.09166374 0.13204384 0.06131313 0.1483699  0.09485362\n"," 0.10092746 0.0943982  0.10530324 0.08204739 0.4215702  0.10784466\n"," 0.25376454 0.1898848  0.12753154 0.0943552  0.07354071 0.708387\n"," 0.12090018 0.06826453 0.06915138 0.7838586  0.24801572 0.09321234\n"," 0.27829152 0.15300193 0.13518834 0.45169005 0.12666544 0.5333545\n"," 0.20425132 0.4767971  0.12413329 0.61613256 0.6049739  0.3872804\n"," 0.12980065], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 1.]\n","Prediction: [0.19800021 0.20895936 0.12237402 0.11733212 0.14814526 0.30115542\n"," 0.11860517 0.42471522 0.15420467 0.5351557  0.23447451 0.12823148\n"," 0.16875271 0.21404803 0.1600216  0.10630716 0.13448617 0.1396495\n"," 0.15682355 0.15707749 0.13007268 0.13699351 0.3917186  0.14942987\n"," 0.32241958 0.2539591  0.19133848 0.11864    0.12019102 0.68370324\n"," 0.1544605  0.12787408 0.12384204 0.6803832  0.22772546 0.11023489\n"," 0.3430998  0.17529953 0.16499853 0.3187498  0.19156727 0.6110058\n"," 0.23965041 0.33640853 0.14432389 0.6759354  0.54097444 0.5498142\n"," 0.14065164], Label: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n"," 0.]\n","Prediction: [0.08783781 0.15077944 0.06852885 0.06851272 0.0663509  0.35814884\n"," 0.06518488 0.20802933 0.08179017 0.5239318  0.15346774 0.05576621\n"," 0.0547394  0.06777866 0.07753006 0.0553217  0.10862781 0.07127733\n"," 0.07746642 0.05761119 0.06683344 0.06435954 0.4397951  0.07721789\n"," 0.19879581 0.14463726 0.08674397 0.06371026 0.07983398 0.8023111\n"," 0.12287188 0.04170141 0.07744229 0.7878359  0.17234464 0.05407286\n"," 0.17993113 0.11628505 0.06703688 0.4360224  0.09408449 0.50372213\n"," 0.23857331 0.6017162  0.07918419 0.4126224  0.4299191  0.34757817\n"," 0.10170516], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n"," 0.]\n","Prediction: [0.06612305 0.0503366  0.03092212 0.01591171 0.02461803 0.25690544\n"," 0.03053035 0.15919207 0.03234304 0.72109294 0.07288044 0.02333915\n"," 0.02642787 0.02508348 0.05045631 0.01648821 0.04434592 0.02352097\n"," 0.03191397 0.02143851 0.01626553 0.0184485  0.60561526 0.01811445\n"," 0.14139253 0.07969767 0.02648281 0.03448186 0.0272265  0.88014394\n"," 0.05354875 0.01872045 0.02761932 0.86381155 0.14588806 0.02109683\n"," 0.07546229 0.06637962 0.03291737 0.4843637  0.02913569 0.5562043\n"," 0.10771643 0.62743044 0.01905681 0.25377586 0.48061875 0.37774765\n"," 0.05477029], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.\n"," 0.]\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, f1_score\n","import pandas as pd\n","import numpy as np\n","\n","class PAR_Dataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, label_file, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations(annotation_file)\n","        self.labels = self.load_labels(label_file)\n","        self.image_paths = {os.path.splitext(f)[0]: os.path.join(image_dir, f) for f in os.listdir(image_dir)}\n","\n","    def load_annotations(self, annotation_file):\n","        with open(annotation_file, 'r') as f:\n","            lines = f.readlines()\n","        annotations = []\n","        for line in lines,:\n","            parts = line.strip().split()\n","            image_name = parts[0]\n","            attributes = list(map(int, parts[1:]))\n","            annotations.append((image_name, attributes))\n","        return annotations\n","\n","    def load_labels(self, label_file):\n","        with open(label_file, 'r') as f:\n","            labels = [line.strip() for line in f.readlines()]\n","        return labels\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name, attributes = self.annotations[idx]\n","        img_path = self.image_paths[image_name]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        attributes = torch.tensor(attributes, dtype=torch.float32)\n","        return image, attributes, image_name\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","class PAR_Model(nn.Module):\n","    def __init__(self, num_attributes):\n","        super(PAR_Model, self).__init__()\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_attributes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","num_epochs = 1\n","learning_rate = 0.0001\n","batch_size = 16\n","\n","train_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=train_transform)\n","val_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model, loss function, and optimizer\n","num_attributes = len(train_dataset.labels)\n","model = PAR_Model(num_attributes)\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, labels, _ in train_loader:  # Ignore image names during training\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_outputs = []\n","    image_names = []\n","\n","    with torch.no_grad():\n","        for images, labels, img_names in val_loader:\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_outputs.extend(outputs.cpu().numpy())\n","            image_names.extend(img_names)\n","\n","    val_loss /= len(val_loader)\n","    scheduler.step()\n","\n","    all_labels = np.array(all_labels)\n","    all_outputs = np.array(all_outputs)\n","    val_acc = accuracy_score(all_labels, all_outputs > 0.5)\n","    val_f1 = f1_score(all_labels, all_outputs > 0.5, average='weighted')\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n","\n","    # Debugging: Print a sample of predictions, corresponding labels, and image names\n","    sample_size = 5\n","    print(\"Sample predictions, labels, and image names:\")\n","    for i in range(sample_size):\n","        print(f\"Image: {image_names[i]}, Prediction: {all_outputs[i]}, Label: {all_labels[i]}\")\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'par_model_resnet50_best.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"EngSudZHk5J_","executionInfo":{"status":"error","timestamp":1717008114010,"user_tz":-330,"elapsed":850,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"8cce581d-f7f5-42dc-9326-cc1d4dd2f875"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'list' object has no attribute 'strip'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-8525cdab77c3>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPAR_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPAR_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-8525cdab77c3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_dir, annotation_file, label_file, transform)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-8525cdab77c3>\u001b[0m in \u001b[0;36mload_annotations\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"]}]},{"cell_type":"code","source":["import os\n","import torch\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, f1_score\n","import pandas as pd\n","import numpy as np\n","\n","class PAR_Dataset(Dataset):\n","    def __init__(self, image_dir, annotation_file, label_file, transform=None):\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.annotations = self.load_annotations(annotation_file)\n","        self.labels = self.load_labels(label_file)\n","        self.image_paths = {os.path.splitext(f)[0]: os.path.join(image_dir, f) for f in os.listdir(image_dir)}\n","\n","    def load_annotations(self, annotation_file):\n","        with open(annotation_file, 'r') as f:\n","            lines = f.readlines()\n","        annotations = []\n","        for line in lines:\n","            parts = line.strip().split()\n","            image_name = parts[0]\n","            attributes = list(map(int, parts[1:]))\n","            annotations.append((image_name, attributes))\n","        return annotations\n","\n","    def load_labels(self, label_file):\n","        with open(label_file, 'r') as f:\n","            labels = [line.strip() for line in f.readlines()]\n","        return labels\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        image_name, attributes = self.annotations[idx]\n","        img_path = self.image_paths[image_name]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        attributes = torch.tensor(attributes, dtype=torch.float32)\n","        return image, attributes, image_name\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","class PAR_Model(nn.Module):\n","    def __init__(self, num_attributes):\n","        super(PAR_Model, self).__init__()\n","        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_attributes)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","num_epochs = 20\n","learning_rate = 0.0001\n","batch_size = 16\n","\n","train_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/images', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/train.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=train_transform)\n","val_dataset = PAR_Dataset('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation_image', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/validation.txt', '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR_new/label.txt', transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Model, loss function, and optimizer\n","num_attributes = len(train_dataset.labels)\n","model = PAR_Model(num_attributes)\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","best_val_loss = float('inf')\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, labels, _ in train_loader:  # Ignore image names during training\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_outputs = []\n","    image_names = []\n","\n","    with torch.no_grad():\n","        for images, labels, img_names in val_loader:\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_outputs.extend(outputs.cpu().numpy())\n","            image_names.extend(img_names)\n","\n","    val_loss /= len(val_loader)\n","    scheduler.step()\n","\n","    all_labels = np.array(all_labels)\n","    all_outputs = np.array(all_outputs)\n","    val_acc = accuracy_score(all_labels, all_outputs > 0.5)\n","    val_f1 = f1_score(all_labels, all_outputs > 0.5, average='weighted')\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}')\n","\n","    # Debugging: Print a sample of predictions, corresponding labels, and image names\n","    sample_size = 5\n","    print(\"Sample predictions, labels, and image names:\")\n","    for i in range(sample_size):\n","        print(f\"Image: {image_names[i]}, Prediction: {all_outputs[i]}, Label: {all_labels[i]}\")\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), 'par_model_resnet50_best.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aR_eaKdRk5Oo","executionInfo":{"status":"error","timestamp":1717008797919,"user_tz":-330,"elapsed":565527,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"2a4af9f1-0114-4189-dea3-6df6f98b1115"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Train Loss: 0.5614, Val Loss: 0.4663, Val Acc: 0.0000, Val F1: 0.1556\n","Sample predictions, labels, and image names:\n","Image: 415, Prediction: [0.13004978 0.11572918 0.11191966 0.14708978 0.07257079 0.1762462\n"," 0.10915355 0.17968765 0.1335721  0.49178645 0.11886345 0.06997617\n"," 0.08680376 0.10277146 0.07320983 0.12374671 0.12134406 0.09477093\n"," 0.10212959 0.09244028 0.11991879 0.07119675 0.3126276  0.08419721\n"," 0.28306064 0.11264478 0.11020786 0.11107735 0.12094741 0.7257078\n"," 0.07665201 0.06653494 0.06685701 0.79241073 0.16382715 0.08299407\n"," 0.2149164  0.07412721 0.07453158 0.33236122 0.10644775 0.4403955\n"," 0.15318787 0.56889087 0.16224137 0.5491767  0.4973974  0.26749355\n"," 0.12166075], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n"," 0.]\n","Image: 416, Prediction: [0.21046665 0.14980501 0.19944511 0.21257415 0.22670232 0.27365467\n"," 0.20008762 0.27162257 0.18007429 0.5471615  0.23492834 0.2024154\n"," 0.17452419 0.16160615 0.27630314 0.16960981 0.24864294 0.1569662\n"," 0.20025419 0.2335458  0.2530129  0.18923599 0.3729682  0.17124535\n"," 0.29769555 0.22680557 0.21935599 0.18881172 0.19332023 0.50144887\n"," 0.21372774 0.2236082  0.23543422 0.6587442  0.27638468 0.15708011\n"," 0.3050667  0.22252813 0.19895545 0.34625384 0.23487766 0.479648\n"," 0.26927292 0.48120388 0.22960652 0.52683806 0.5697928  0.30128235\n"," 0.26659352], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n"," 1.]\n","Image: 417, Prediction: [0.16159387 0.09794234 0.10469546 0.12174225 0.14279906 0.20062007\n"," 0.184526   0.20954257 0.1531285  0.6053435  0.13299982 0.12379141\n"," 0.12120382 0.1388936  0.16553216 0.15799303 0.13789745 0.14417432\n"," 0.15488757 0.11261981 0.13288082 0.08105819 0.30417314 0.09184043\n"," 0.23845255 0.19761844 0.17449652 0.1115602  0.12121372 0.6362277\n"," 0.15232787 0.10552619 0.20210692 0.7877395  0.19766726 0.18124898\n"," 0.24327035 0.14498836 0.19475324 0.29997388 0.18790801 0.45570338\n"," 0.1819955  0.47140878 0.2450603  0.479909   0.4670847  0.37344268\n"," 0.19897752], Label: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n"," 0.]\n","Image: 418, Prediction: [0.13588756 0.09355424 0.14312083 0.12592818 0.1087149  0.21450809\n"," 0.10852344 0.2212986  0.09864399 0.509699   0.11893616 0.12739114\n"," 0.15476859 0.10570833 0.16295977 0.10450675 0.11396879 0.14638211\n"," 0.1312081  0.1219404  0.1304997  0.07296839 0.27821314 0.10081686\n"," 0.27598906 0.11802913 0.14612514 0.19176717 0.10326436 0.6143527\n"," 0.09226363 0.12873277 0.13223343 0.68323416 0.20756035 0.09075918\n"," 0.2373673  0.12975793 0.15226424 0.34839693 0.14102693 0.4296034\n"," 0.17630896 0.46279034 0.19155437 0.54048157 0.44807342 0.25400084\n"," 0.11487098], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n"," 0.]\n","Image: 419, Prediction: [0.04761491 0.06339896 0.04808993 0.06723242 0.04807846 0.14636792\n"," 0.06456266 0.10814613 0.04624894 0.6846191  0.08713571 0.0256971\n"," 0.05252557 0.04958527 0.06102962 0.04008858 0.04833629 0.04091996\n"," 0.0270224  0.05237506 0.04316755 0.03388755 0.30606133 0.04004872\n"," 0.14736776 0.05671937 0.05859229 0.06087137 0.04340255 0.77294207\n"," 0.05101325 0.02994163 0.03970313 0.78783256 0.06446644 0.05207912\n"," 0.12326735 0.05480025 0.10738711 0.47409213 0.06441253 0.51106673\n"," 0.07061797 0.60621744 0.07792101 0.41584548 0.5213226  0.27164507\n"," 0.05636491], Label: [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.\n"," 0.]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-c7e606f72b9e>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"10oM5eoslxUO7uCTzTuLOYg4WHrIgQYrY","authorship_tag":"ABX9TyOAbokM6mlXNFblBKJkKNOV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}