{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1aKzxmUGP56UGgq-Xfk727flYoB9qyZsm","authorship_tag":"ABX9TyNFCgugCdaAyD0/aqA43BMl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/VRL_challenge_PAR/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Instantiate and compile the model\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Save the trained model\n","model_path = \"resnet_model.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n","\n","# Validation with mean accuracy calculation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","        total += targets.size(0) * NUM_CLASSES\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / total\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')"],"metadata":{"id":"ZfdL7_u4PUsE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Instantiate and compile the model\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Save the trained model\n","model_path = \"resnet_model.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n","\n","# Validation with mean accuracy and MAE calculation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","total = 0\n","total_mae = 0.0\n","label_mae = np.zeros(NUM_CLASSES)\n","\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","        total += targets.size(0) * NUM_CLASSES\n","\n","        mae = torch.abs(preds - targets).sum(dim=0)  # MAE for each label\n","        label_mae += mae.cpu().numpy()\n","        total_mae += mae.sum().item()  # Overall MAE\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / total\n","overall_mae = total_mae / total\n","label_mae = label_mae / (len(val_loader.dataset) / BATCH_SIZE)  # Average MAE per label\n","\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n","print(f'Overall MAE: {overall_mae:.4f}')\n","print(f'Label-wise MAE: {label_mae}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rezkYFHnPzlp","executionInfo":{"status":"ok","timestamp":1718645688013,"user_tz":-330,"elapsed":282736,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"2cf77c8e-b563-4534-d068-b9c0f87afd65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 0.4317\n","Epoch 2/50, Loss: 0.3630\n","Epoch 3/50, Loss: 0.3348\n","Epoch 4/50, Loss: 0.3224\n","Epoch 5/50, Loss: 0.3100\n","Epoch 6/50, Loss: 0.3007\n","Epoch 7/50, Loss: 0.2874\n","Epoch 8/50, Loss: 0.2811\n","Epoch 9/50, Loss: 0.2708\n","Epoch 10/50, Loss: 0.2643\n","Epoch 11/50, Loss: 0.2569\n","Epoch 12/50, Loss: 0.2431\n","Epoch 13/50, Loss: 0.2399\n","Epoch 14/50, Loss: 0.2332\n","Epoch 15/50, Loss: 0.2286\n","Epoch 16/50, Loss: 0.2185\n","Epoch 17/50, Loss: 0.2153\n","Epoch 18/50, Loss: 0.2048\n","Epoch 19/50, Loss: 0.2014\n","Epoch 20/50, Loss: 0.2028\n","Epoch 21/50, Loss: 0.1961\n","Epoch 22/50, Loss: 0.1923\n","Epoch 23/50, Loss: 0.1827\n","Epoch 24/50, Loss: 0.1834\n","Epoch 25/50, Loss: 0.1795\n","Epoch 26/50, Loss: 0.1775\n","Epoch 27/50, Loss: 0.1700\n","Epoch 28/50, Loss: 0.1712\n","Epoch 29/50, Loss: 0.1687\n","Epoch 30/50, Loss: 0.1587\n","Epoch 31/50, Loss: 0.1537\n","Epoch 32/50, Loss: 0.1518\n","Epoch 33/50, Loss: 0.1474\n","Epoch 34/50, Loss: 0.1454\n","Epoch 35/50, Loss: 0.1422\n","Epoch 36/50, Loss: 0.1355\n","Epoch 37/50, Loss: 0.1327\n","Epoch 38/50, Loss: 0.1381\n","Epoch 39/50, Loss: 0.1356\n","Epoch 40/50, Loss: 0.1291\n","Epoch 41/50, Loss: 0.1293\n","Epoch 42/50, Loss: 0.1243\n","Epoch 43/50, Loss: 0.1238\n","Epoch 44/50, Loss: 0.1246\n","Epoch 45/50, Loss: 0.1171\n","Epoch 46/50, Loss: 0.1126\n","Epoch 47/50, Loss: 0.1185\n","Epoch 48/50, Loss: 0.1144\n","Epoch 49/50, Loss: 0.1162\n","Epoch 50/50, Loss: 0.1134\n","Model saved to resnet_model.pth\n","Validation Loss: 0.3070\n","Validation Accuracy: 0.8878\n","Overall MAE: 0.1122\n","Label-wise MAE: [ 2.4         2.93333333  1.06666667  1.33333333  1.33333333  5.06666667\n","  2.66666667  5.06666667  3.73333333  6.4         2.13333333  0.8\n","  1.6         1.6         3.73333333  1.06666667  2.93333333  1.33333333\n","  2.4         1.86666667  4.          1.6         4.8         5.6\n","  4.8         2.93333333  1.86666667  4.          2.13333333  4.26666667\n","  2.66666667  2.66666667  0.8         6.93333333  6.93333333  2.93333333\n","  4.26666667  3.73333333  2.4         7.46666667  3.2        11.2\n"," 10.13333333  2.93333333  0.26666667  2.66666667  6.13333333  6.4\n","  4.8       ]\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '//content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'train.txt')\n","images_folder = os.path.join(drive_base_path, 'images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Instantiate and compile the model\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Save the trained model\n","model_path = \"resnet_model.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n","\n","# Validation with mean accuracy and MAE calculation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","total = 0\n","total_mae = 0.0\n","label_mae = np.zeros(NUM_CLASSES)\n","num_samples = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","        total += targets.size(0) * NUM_CLASSES\n","\n","        mae = torch.abs(preds - targets).sum(dim=0)  # MAE for each label\n","        label_mae += mae.cpu().numpy()\n","        total_mae += mae.sum().item()  # Overall MAE\n","        num_samples += targets.size(0)\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / total\n","overall_mae = total_mae / total\n","label_mae = label_mae / num_samples  # Average MAE per label\n","\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n","print(f'Overall MAE: {overall_mae:.4f}')\n","print(f'Label-wise MAE: {label_mae}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpIoVe2NRfN-","executionInfo":{"status":"ok","timestamp":1718646131421,"user_tz":-330,"elapsed":279646,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"96a24d4c-dcfc-4ec8-fa21-3284afb64f6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 0.4454\n","Epoch 2/50, Loss: 0.3705\n","Epoch 3/50, Loss: 0.3475\n","Epoch 4/50, Loss: 0.3329\n","Epoch 5/50, Loss: 0.3222\n","Epoch 6/50, Loss: 0.3144\n","Epoch 7/50, Loss: 0.3099\n","Epoch 8/50, Loss: 0.3028\n","Epoch 9/50, Loss: 0.3005\n","Epoch 10/50, Loss: 0.2857\n","Epoch 11/50, Loss: 0.2761\n","Epoch 12/50, Loss: 0.2702\n","Epoch 13/50, Loss: 0.2618\n","Epoch 14/50, Loss: 0.2566\n","Epoch 15/50, Loss: 0.2545\n","Epoch 16/50, Loss: 0.2515\n","Epoch 17/50, Loss: 0.2398\n","Epoch 18/50, Loss: 0.2377\n","Epoch 19/50, Loss: 0.2286\n","Epoch 20/50, Loss: 0.2176\n","Epoch 21/50, Loss: 0.2164\n","Epoch 22/50, Loss: 0.2056\n","Epoch 23/50, Loss: 0.2041\n","Epoch 24/50, Loss: 0.2007\n","Epoch 25/50, Loss: 0.1979\n","Epoch 26/50, Loss: 0.1890\n","Epoch 27/50, Loss: 0.1863\n","Epoch 28/50, Loss: 0.1828\n","Epoch 29/50, Loss: 0.1829\n","Epoch 30/50, Loss: 0.1769\n","Epoch 31/50, Loss: 0.1690\n","Epoch 32/50, Loss: 0.1702\n","Epoch 33/50, Loss: 0.1705\n","Epoch 34/50, Loss: 0.1623\n","Epoch 35/50, Loss: 0.1576\n","Epoch 36/50, Loss: 0.1552\n","Epoch 37/50, Loss: 0.1488\n","Epoch 38/50, Loss: 0.1438\n","Epoch 39/50, Loss: 0.1442\n","Epoch 40/50, Loss: 0.1433\n","Epoch 41/50, Loss: 0.1395\n","Epoch 42/50, Loss: 0.1425\n","Epoch 43/50, Loss: 0.1364\n","Epoch 44/50, Loss: 0.1308\n","Epoch 45/50, Loss: 0.1338\n","Epoch 46/50, Loss: 0.1352\n","Epoch 47/50, Loss: 0.1284\n","Epoch 48/50, Loss: 0.1297\n","Epoch 49/50, Loss: 0.1218\n","Epoch 50/50, Loss: 0.1202\n","Model saved to resnet_model.pth\n","Validation Loss: 0.3317\n","Validation Accuracy: 0.8762\n","Overall MAE: 0.1238\n","Label-wise MAE: [0.10833333 0.09166667 0.05833333 0.05       0.06666667 0.1\n"," 0.025      0.16666667 0.15       0.25       0.13333333 0.075\n"," 0.03333333 0.04166667 0.075      0.025      0.10833333 0.04166667\n"," 0.06666667 0.06666667 0.10833333 0.05833333 0.19166667 0.11666667\n"," 0.24166667 0.09166667 0.06666667 0.10833333 0.09166667 0.175\n"," 0.1        0.11666667 0.025      0.25833333 0.24166667 0.075\n"," 0.18333333 0.13333333 0.11666667 0.275      0.10833333 0.30833333\n"," 0.28333333 0.10833333 0.01666667 0.1        0.3        0.18333333\n"," 0.15      ]\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, hamming_loss, accuracy_score\n","\n","# Define constants\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","NUM_CLASSES = 49\n","BATCH_SIZE = 32\n","EPOCHS = 50\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/'\n","train_path = os.path.join(drive_base_path, 'path_to_your_output_file.txt')\n","images_folder = os.path.join(drive_base_path, 'augmented_images')\n","\n","# Load dataset\n","train_df = pd.read_csv(train_path, sep=' ', header=None)\n","image_names = train_df.iloc[:, 0].astype(str).values\n","labels = train_df.iloc[:, 1:].values.astype(int)\n","\n","# Split dataset\n","image_names_train, image_names_val, labels_train, labels_val = train_test_split(image_names, labels, test_size=0.2, random_state=42)\n","\n","# Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, image_names, labels, images_folder, transform=None):\n","        self.image_names = image_names\n","        self.labels = labels\n","        self.images_folder = images_folder\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.image_names[idx]\n","        img_path = os.path.join(self.images_folder, f\"{img_name}.jpg\")\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        label = self.labels[idx]\n","        return image, label\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader\n","train_dataset = CustomDataset(image_names_train, labels_train, images_folder, transform=transform)\n","val_dataset = CustomDataset(image_names_val, labels_val, images_folder, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Instantiate and compile the model\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item() * inputs.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}')\n","\n","# Save the trained model\n","model_path = \"resnet_model.pth\"\n","torch.save(model.state_dict(), model_path)\n","print(f\"Model saved to {model_path}\")\n","\n","# Validation with metrics calculation\n","model.eval()\n","val_loss = 0.0\n","val_corrects = 0\n","total = 0\n","total_mae = 0.0\n","label_mae = np.zeros(NUM_CLASSES)\n","num_samples = 0\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE).float()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        val_loss += loss.item() * inputs.size(0)\n","\n","        preds = (outputs > 0.5).float()\n","        val_corrects += (preds == targets).float().sum()\n","        total += targets.size(0) * NUM_CLASSES\n","\n","        mae = torch.abs(preds - targets).sum(dim=0)  # MAE for each label\n","        label_mae += mae.cpu().numpy()\n","        total_mae += mae.sum().item()  # Overall MAE\n","        num_samples += targets.size(0)\n","\n","        all_preds.append(preds.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","all_preds = np.vstack(all_preds)\n","all_targets = np.vstack(all_targets)\n","\n","val_loss = val_loss / len(val_loader.dataset)\n","val_accuracy = val_corrects / total\n","overall_mae = total_mae / total\n","label_mae = label_mae / num_samples  # Average MAE per label\n","\n","# Calculate additional metrics\n","precision = precision_score(all_targets, all_preds, average='micro')\n","recall = recall_score(all_targets, all_preds, average='micro')\n","f1 = f1_score(all_targets, all_preds, average='micro')\n","roc_auc = roc_auc_score(all_targets, all_preds, average='micro')\n","hamming = hamming_loss(all_targets, all_preds)\n","subset_acc = accuracy_score(all_targets, all_preds)\n","\n","print(f'Validation Loss: {val_loss:.4f}')\n","print(f'Validation Accuracy: {val_accuracy:.4f}')\n","print(f'Overall MAE: {overall_mae:.4f}')\n","print(f'Label-wise MAE: {label_mae}')\n","print(f'Precision: {precision:.4f}')\n","print(f'Recall: {recall:.4f}')\n","print(f'F1 Score: {f1:.4f}')\n","print(f'ROC-AUC: {roc_auc:.4f}')\n","print(f'Hamming Loss: {hamming:.4f}')\n","print(f'Subset Accuracy: {subset_acc:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7QUfLAD9UiKs","executionInfo":{"status":"ok","timestamp":1718991430227,"user_tz":-330,"elapsed":3869264,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"460db523-b8d6-42e5-8f05-22d87f5fe50e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 102MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 0.3398\n","Epoch 2/50, Loss: 0.2793\n","Epoch 3/50, Loss: 0.2422\n","Epoch 4/50, Loss: 0.2144\n","Epoch 5/50, Loss: 0.1878\n","Epoch 6/50, Loss: 0.1718\n","Epoch 7/50, Loss: 0.1554\n","Epoch 8/50, Loss: 0.1400\n","Epoch 9/50, Loss: 0.1296\n","Epoch 10/50, Loss: 0.1180\n","Epoch 11/50, Loss: 0.1082\n","Epoch 12/50, Loss: 0.0994\n","Epoch 13/50, Loss: 0.0911\n","Epoch 14/50, Loss: 0.0851\n","Epoch 15/50, Loss: 0.0790\n","Epoch 16/50, Loss: 0.0749\n","Epoch 17/50, Loss: 0.0689\n","Epoch 18/50, Loss: 0.0660\n","Epoch 19/50, Loss: 0.0607\n","Epoch 20/50, Loss: 0.0573\n","Epoch 21/50, Loss: 0.0557\n","Epoch 22/50, Loss: 0.0516\n","Epoch 23/50, Loss: 0.0486\n","Epoch 24/50, Loss: 0.0479\n","Epoch 25/50, Loss: 0.0443\n","Epoch 26/50, Loss: 0.0423\n","Epoch 27/50, Loss: 0.0435\n","Epoch 28/50, Loss: 0.0406\n","Epoch 29/50, Loss: 0.0364\n","Epoch 30/50, Loss: 0.0373\n","Epoch 31/50, Loss: 0.0362\n","Epoch 32/50, Loss: 0.0338\n","Epoch 33/50, Loss: 0.0366\n","Epoch 34/50, Loss: 0.0333\n","Epoch 35/50, Loss: 0.0320\n","Epoch 36/50, Loss: 0.0285\n","Epoch 37/50, Loss: 0.0284\n","Epoch 38/50, Loss: 0.0277\n","Epoch 39/50, Loss: 0.0288\n","Epoch 40/50, Loss: 0.0287\n","Epoch 41/50, Loss: 0.0311\n","Epoch 42/50, Loss: 0.0240\n","Epoch 43/50, Loss: 0.0241\n","Epoch 44/50, Loss: 0.0232\n","Epoch 45/50, Loss: 0.0246\n","Epoch 46/50, Loss: 0.0228\n","Epoch 47/50, Loss: 0.0211\n","Epoch 48/50, Loss: 0.0211\n","Epoch 49/50, Loss: 0.0203\n","Epoch 50/50, Loss: 0.0202\n","Model saved to resnet_model.pth\n","Validation Loss: 0.0097\n","Validation Accuracy: 0.9972\n","Overall MAE: 0.0028\n","Label-wise MAE: [0.00083333 0.005      0.0025     0.         0.         0.00166667\n"," 0.00083333 0.00333333 0.00083333 0.00916667 0.00916667 0.\n"," 0.00083333 0.00083333 0.00333333 0.         0.00333333 0.\n"," 0.00333333 0.         0.00166667 0.         0.00416667 0.00166667\n"," 0.00666667 0.00333333 0.0025     0.0025     0.         0.005\n"," 0.00166667 0.         0.00083333 0.0025     0.00666667 0.00083333\n"," 0.00166667 0.00916667 0.00083333 0.00583333 0.00166667 0.01083333\n"," 0.0075     0.00166667 0.00166667 0.00333333 0.0025     0.00083333\n"," 0.00333333]\n","Precision: 0.9959\n","Recall: 0.9881\n","F1 Score: 0.9920\n","ROC-AUC: 0.9936\n","Hamming Loss: 0.0028\n","Subset Accuracy: 0.9267\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VblXEz_od0W","executionInfo":{"status":"ok","timestamp":1718987450455,"user_tz":-330,"elapsed":47024,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"eb49c5e4-1944-4523-e6bc-fd831bbe4371"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Define constants\n","NUM_CLASSES = 49\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the saved model\n","model_path = \"/content/resnet_model.pth\"\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n","model.eval()\n","\n","# Define the image transformation\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Function to predict labels for a new image\n","def predict_image(image_path):\n","    # Load and transform the image\n","    image = Image.open(image_path).convert(\"RGB\")\n","    image = transform(image).unsqueeze(0).to(DEVICE)\n","\n","    # Predict label\n","    with torch.no_grad():\n","        outputs = model(image)\n","        preds = (outputs > 0.5).float().cpu().numpy()\n","\n","    return preds\n","\n","# Example usage\n","new_image_path = os.path.join('/content/drive/MyDrive/ANSYS/VRL_challenge_PAR1/VRL_challenge_PAR/images', '10.jpg')  # Path to the new image\n","predictions = predict_image(new_image_path)\n","print(f\"Predicted labels for the new image: {predictions}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDQJMOOOWarE","executionInfo":{"status":"ok","timestamp":1718991467756,"user_tz":-330,"elapsed":1561,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"bdf024cc-8d27-49d2-dab6-c16501e39411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Predicted labels for the new image: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n","  1.]]\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from PIL import Image\n","import os\n","import glob\n","\n","# Define constants\n","NUM_CLASSES = 49\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","TEST_BATCH_SIZE = 1  # Batch size for test predictions\n","\n","# Define paths\n","drive_base_path = '/content/drive/MyDrive/Colab Notebooks/VRL_challenge_PAR/VRL_challenge_PAR/'\n","test_folder = os.path.join(drive_base_path, 'test_images')\n","\n","# Custom Dataset class for the test set\n","class TestDataset(Dataset):\n","    def __init__(self, image_paths, transform=None):\n","        self.image_paths = image_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, os.path.basename(img_path)\n","\n","# Get all image paths in the test folder\n","test_image_paths = glob.glob(os.path.join(test_folder, '*.jpg'))\n","\n","# Define transforms for the test images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create DataLoader for test dataset\n","test_dataset = TestDataset(test_image_paths, transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","# Define the ResNet model\n","class ResNetModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNetModel, self).__init__()\n","        self.resnet = models.resnet50(pretrained=True)\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(self.resnet.fc.in_features, 512),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)\n","\n","# Instantiate the model and load the saved weights\n","model = ResNetModel(NUM_CLASSES).to(DEVICE)\n","model_path = \"/content/resnet_model.pth\"\n","model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n","model.eval()\n","\n","# Function to predict labels for the test dataset\n","def predict_test_dataset(test_loader, model, device):\n","    model.eval()\n","    predictions = []\n","    image_names = []\n","\n","    with torch.no_grad():\n","        for images, img_names in test_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            preds = (outputs > 0.5).float().cpu().numpy().squeeze()  # Squeeze to remove batch dimension\n","            predictions.append(preds)\n","            image_names.extend(img_names)\n","\n","    return predictions, image_names\n","\n","# Make predictions\n","test_predictions, test_image_names = predict_test_dataset(test_loader, model, DEVICE)\n","\n","# Save predictions to a text file\n","output_file = \"test_predictions.txt\"\n","with open(output_file, 'w') as f:\n","    for img_name, preds in zip(test_image_names, test_predictions):\n","        pred_str = \",\".join([str(int(pred)) for pred in preds])\n","        f.write(f\"{img_name}, {pred_str}\\n\")\n","\n","print(f\"Predictions saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hk2YPdhYiFV","executionInfo":{"status":"ok","timestamp":1718647668511,"user_tz":-330,"elapsed":2124,"user":{"displayName":"CHANDAN RAJ","userId":"01959545729343590656"}},"outputId":"e343d89f-8ac8-4f4e-fbc1-c5a2a4fdf609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to test_predictions.txt\n"]}]}]}